{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "forty-filename",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "applied-workshop",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Collection\n",
    "#Goal: Organize your data to streamline the next steps of your\n",
    "#capstone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cellular-element",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data source:\n",
      "    https://www.kaggle.com/andrewmvd/okcupid-profiles\n"
     ]
    }
   ],
   "source": [
    "print('''The data source:\n",
    "    https://www.kaggle.com/andrewmvd/okcupid-profiles''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dynamic-uncertainty",
   "metadata": {},
   "outputs": [],
   "source": [
    "dating_data = pd.read_csv('okcupid_profiles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "nonprofit-guatemala",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age  status sex orientation       body_type               diet    drinks  \\\n",
      "0   22  single   m    straight  a little extra  strictly anything  socially   \n",
      "1   35  single   m    straight         average       mostly other     often   \n",
      "\n",
      "       drugs                      education     ethnicity  ...  \\\n",
      "0      never  working on college/university  asian, white  ...   \n",
      "1  sometimes          working on space camp         white  ...   \n",
      "\n",
      "                                              essay0  \\\n",
      "0  about me:  i would love to think that i was so...   \n",
      "1  i am a chef: this is what that means. 1. i am ...   \n",
      "\n",
      "                                              essay1  \\\n",
      "0  currently working as an international agent fo...   \n",
      "1  dedicating everyday to being an unbelievable b...   \n",
      "\n",
      "                                              essay2  \\\n",
      "0  making people laugh. ranting about a good salt...   \n",
      "1  being silly. having ridiculous amonts of fun w...   \n",
      "\n",
      "                                              essay3  \\\n",
      "0  the way i look. i am a six foot half asian, ha...   \n",
      "1                                                NaN   \n",
      "\n",
      "                                              essay4  \\\n",
      "0  books: absurdistan, the republic, of mice and ...   \n",
      "1  i am die hard christopher moore fan. i don't r...   \n",
      "\n",
      "                                              essay5  \\\n",
      "0                  food. water. cell phone. shelter.   \n",
      "1  delicious porkness in all of its glories. my b...   \n",
      "\n",
      "                        essay6  \\\n",
      "0  duality and humorous things   \n",
      "1                          NaN   \n",
      "\n",
      "                                              essay7  \\\n",
      "0  trying to find someone to hang out with. i am ...   \n",
      "1                                                NaN   \n",
      "\n",
      "                                              essay8  \\\n",
      "0  i am new to california and looking for someone...   \n",
      "1  i am very open and will share just about anyth...   \n",
      "\n",
      "                                              essay9  \n",
      "0  you want to be swept off your feet! you are ti...  \n",
      "1                                                NaN  \n",
      "\n",
      "[2 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "print(dating_data.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "proof-result",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Organization\n",
    "# Goal: Create a file structure and add your work to the GitHub\n",
    "#repository youâ€™ve created for this project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adverse-butterfly",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The repo is here\n",
      "https://github.com/Prathmun/DatingDataCapstone\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('''The repo is here\n",
    "https://github.com/Prathmun/DatingDataCapstone\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "rubber-costume",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age: is a: <class 'numpy.int64'> and it has *54* unique values\n",
      "status: is a: <class 'str'> and it has *5* unique values\n",
      "sex: is a: <class 'str'> and it has *2* unique values\n",
      "orientation: is a: <class 'str'> and it has *3* unique values\n",
      "body_type: is a: <class 'str'> and it has *13* unique values\n",
      "diet: is a: <class 'str'> and it has *19* unique values\n",
      "drinks: is a: <class 'str'> and it has *7* unique values\n",
      "drugs: is a: <class 'float'> and it has *4* unique values\n",
      "education: is a: <class 'str'> and it has *33* unique values\n",
      "ethnicity: is a: <class 'float'> and it has *218* unique values\n",
      "height: is a: <class 'numpy.float64'> and it has *61* unique values\n",
      "income: is a: <class 'numpy.int64'> and it has *13* unique values\n",
      "job: is a: <class 'float'> and it has *22* unique values\n",
      "last_online: is a: <class 'str'> and it has *30123* unique values\n",
      "location: is a: <class 'str'> and it has *199* unique values\n",
      "offspring: is a: <class 'float'> and it has *16* unique values\n",
      "pets: is a: <class 'str'> and it has *16* unique values\n",
      "religion: is a: <class 'float'> and it has *46* unique values\n",
      "sign: is a: <class 'str'> and it has *49* unique values\n",
      "smokes: is a: <class 'str'> and it has *6* unique values\n",
      "speaks: is a: <class 'str'> and it has *7648* unique values\n"
     ]
    }
   ],
   "source": [
    "for header in dating_data.columns:\n",
    "    if 'essay' not in header:\n",
    "        print(header + \": is a: \" + str(type(dating_data[header].values[2])) + \" and it has *\" + str(len(pd.unique(dating_data[header])))\n",
    "             + \"* unique values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "exempt-shelter",
   "metadata": {},
   "outputs": [],
   "source": [
    "#don't need those essay columns for this project\n",
    "droplist = []\n",
    "for i in range(10):\n",
    "    appendable = 'essay'+str(i)\n",
    "    droplist.append(appendable)\n",
    "dating_data = dating_data.drop(droplist, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "smooth-mineral",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age              int64\n",
       "status          object\n",
       "sex             object\n",
       "orientation     object\n",
       "body_type       object\n",
       "diet            object\n",
       "drinks          object\n",
       "drugs           object\n",
       "education       object\n",
       "ethnicity       object\n",
       "height         float64\n",
       "income           int64\n",
       "job             object\n",
       "last_online     object\n",
       "location        object\n",
       "offspring       object\n",
       "pets            object\n",
       "religion        object\n",
       "sign            object\n",
       "smokes          object\n",
       "speaks          object\n",
       "dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dating_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "moral-election",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doesn't have kids, but might want them\n",
      "doesn't have kids, but might want them\n",
      "nan\n",
      "doesn't want kids\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "#What kind of data is this, are these categories?\n",
    "for i in range(5):\n",
    "    print(dating_data['offspring'].values[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "exterior-disease",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Yep. Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "floating-domain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "likes dogs and likes cats\n",
      "likes dogs and likes cats\n",
      "has cats\n",
      "likes cats\n",
      "likes dogs and likes cats\n"
     ]
    }
   ],
   "source": [
    "#What kind of data is this, are these categories?\n",
    "for i in range(5):\n",
    "    print(dating_data['pets'].values[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "turkish-wellington",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Also yep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "scheduled-signal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gemini\n",
      "cancer\n",
      "pisces but it doesn&rsquo;t matter\n",
      "pisces\n",
      "aquarius\n"
     ]
    }
   ],
   "source": [
    "#What kind of data is this, are these categories?\n",
    "for i in range(5):\n",
    "    print(dating_data['sign'].values[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "hollow-planning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gemini and it&rsquo;s fun to think about         1782\n",
      "scorpio and it&rsquo;s fun to think about        1772\n",
      "leo and it&rsquo;s fun to think about            1692\n",
      "libra and it&rsquo;s fun to think about          1649\n",
      "taurus and it&rsquo;s fun to think about         1640\n",
      "cancer and it&rsquo;s fun to think about         1597\n",
      "pisces and it&rsquo;s fun to think about         1592\n",
      "sagittarius and it&rsquo;s fun to think about    1583\n",
      "virgo and it&rsquo;s fun to think about          1574\n",
      "aries and it&rsquo;s fun to think about          1573\n",
      "aquarius and it&rsquo;s fun to think about       1503\n",
      "virgo but it doesn&rsquo;t matter                1497\n",
      "leo but it doesn&rsquo;t matter                  1457\n",
      "cancer but it doesn&rsquo;t matter               1454\n",
      "gemini but it doesn&rsquo;t matter               1453\n",
      "taurus but it doesn&rsquo;t matter               1450\n",
      "aquarius but it doesn&rsquo;t matter             1408\n",
      "libra but it doesn&rsquo;t matter                1408\n",
      "capricorn and it&rsquo;s fun to think about      1376\n",
      "sagittarius but it doesn&rsquo;t matter          1375\n",
      "aries but it doesn&rsquo;t matter                1373\n",
      "capricorn but it doesn&rsquo;t matter            1319\n",
      "pisces but it doesn&rsquo;t matter               1300\n",
      "scorpio but it doesn&rsquo;t matter              1264\n",
      "leo                                              1159\n",
      "libra                                            1098\n",
      "cancer                                           1092\n",
      "virgo                                            1029\n",
      "scorpio                                          1020\n",
      "gemini                                           1013\n",
      "taurus                                           1001\n",
      "aries                                             996\n",
      "pisces                                            992\n",
      "aquarius                                          954\n",
      "sagittarius                                       937\n",
      "capricorn                                         833\n",
      "scorpio and it matters a lot                       78\n",
      "leo and it matters a lot                           66\n",
      "cancer and it matters a lot                        63\n",
      "aquarius and it matters a lot                      63\n",
      "gemini and it matters a lot                        62\n",
      "pisces and it matters a lot                        62\n",
      "libra and it matters a lot                         52\n",
      "taurus and it matters a lot                        49\n",
      "aries and it matters a lot                         47\n",
      "sagittarius and it matters a lot                   47\n",
      "capricorn and it matters a lot                     45\n",
      "virgo and it matters a lot                         41\n",
      "Name: sign, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(dating_data['sign'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fiscal-density",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Yep, thankfully more cats not freeform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "hollywood-running",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english\n",
      "english (fluently), spanish (poorly), french (poorly)\n",
      "english, french, c++\n",
      "english, german (poorly)\n",
      "english\n",
      "7648\n"
     ]
    }
   ],
   "source": [
    "#What kind of data is this, are these categories?\n",
    "for i in range(5):\n",
    "    print(dating_data['speaks'].values[i])\n",
    "print(len(pd.unique(dating_data['speaks'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "metropolitan-compound",
   "metadata": {},
   "outputs": [],
   "source": [
    "# looks like cats because spellings are uniform, but more exploration required because the c++ is throwing me off. \n",
    "# Can probably be converted into categories though. There are only so many languages. \n",
    "#The unique count is huge, but with the combinations of possible languages and fluency, I'm still not convinced it can't be a \n",
    "#kind of categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "sustainable-stupid",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    2012-06-28-20-30\n",
      "1    2012-06-29-21-41\n",
      "2    2012-06-27-09-10\n",
      "3    2012-06-28-14-22\n",
      "4    2012-06-27-21-26\n",
      "Name: last_online, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(dating_data['last_online'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "developmental-leadership",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       2012-06-28 20:30:00\n",
      "1       2012-06-29 21:41:00\n",
      "2       2012-06-27 09:10:00\n",
      "3       2012-06-28 14:22:00\n",
      "4       2012-06-27 21:26:00\n",
      "                ...        \n",
      "59941   2012-06-12 21:47:00\n",
      "59942   2012-06-29 11:01:00\n",
      "59943   2012-06-27 23:37:00\n",
      "59944   2012-06-23 13:01:00\n",
      "59945   2012-06-29 00:42:00\n",
      "Name: last_online, Length: 59946, dtype: datetime64[ns]\n",
      "<class 'numpy.datetime64'>\n"
     ]
    }
   ],
   "source": [
    "#converting last online from strings to datetime objects. I kind of doubt we're going to use this column, but it seemed \n",
    "#tidy to convert it.\n",
    "dating_data['last_online'] = pd.to_datetime(dating_data['last_online'], format='%Y-%m-%d-%H-%M')\n",
    "print(dating_data['last_online'])\n",
    "print(type(dating_data['last_online'].values[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "atmospheric-crisis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m    35829\n",
      "f    24117\n",
      "Name: sex, dtype: int64\n",
      "0        m\n",
      "1        m\n",
      "2        m\n",
      "3        m\n",
      "4        m\n",
      "        ..\n",
      "59941    f\n",
      "59942    m\n",
      "59943    m\n",
      "59944    m\n",
      "59945    m\n",
      "Name: sex, Length: 59946, dtype: category\n",
      "Categories (2, object): ['f', 'm']\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "#exploring turning a simple column into categories. IE: sex\n",
    "\n",
    "print(dating_data['sex'].value_counts())\n",
    "dating_data['sex'] = dating_data['sex'].astype('category')\n",
    "\n",
    "print(dating_data['sex'])\n",
    "print(type(dating_data['sex'].values[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "heated-belize",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the cleaner string columns into categories now that we've tested it. Leaving out Ethnicity, location \n",
    "#and speaks for now as they appear more complicated. We'll fiddle with them next. Jobs as well is a little unclear, we'll \n",
    "#explore that shortly as well.\n",
    "\n",
    "dating_data['orientation'] = dating_data['orientation'].astype('category')\n",
    "dating_data['body_type'] = dating_data['body_type'].astype('category')\n",
    "dating_data['diet'] = dating_data['diet'].astype('category')\n",
    "dating_data['drinks'] = dating_data['drinks'].astype('category')\n",
    "dating_data['drugs'] = dating_data['drugs'].astype('category')\n",
    "dating_data['education'] = dating_data['education'].astype('category')\n",
    "dating_data['location'] = dating_data['location'].astype('category')\n",
    "dating_data['pets'] = dating_data['pets'].astype('category')\n",
    "dating_data['sign'] = dating_data['sign'].astype('category')\n",
    "dating_data['smokes'] = dating_data['smokes'].astype('category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "opposite-south",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                     int64\n",
       "status                 object\n",
       "sex                  category\n",
       "orientation          category\n",
       "body_type            category\n",
       "diet                 category\n",
       "drinks               category\n",
       "drugs                category\n",
       "education            category\n",
       "ethnicity              object\n",
       "height                float64\n",
       "income                  int64\n",
       "job                    object\n",
       "last_online    datetime64[ns]\n",
       "location             category\n",
       "offspring              object\n",
       "pets                 category\n",
       "religion               object\n",
       "sign                 category\n",
       "smokes               category\n",
       "speaks                 object\n",
       "dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#just checking to see if the above worked\n",
    "dating_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "going-variation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59946, 21)\n"
     ]
    }
   ],
   "source": [
    "#This was the first place I thought to check this, used it a lot in testing and constructing later functions\n",
    "print(dating_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eight-elimination",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ethical-biotechnology",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'nan': 5680, 'asian': 8205, 'white': 37882, 'black': 3328, 'other': 3567, 'hispanic / latin': 5357, 'pacific islander': 1473, 'native american': 1265, 'middle eastern': 950, 'indian': 1449}\n",
      "{'other', 'asian', 'native american', 'hispanic / latin', 'nan', 'black', 'indian', 'pacific islander', 'middle eastern', 'white'}\n",
      "{'nan': [], 'asian': [], 'white': [], 'black': [], 'other': [], 'hispanic / latin': [], 'pacific islander': [], 'native american': [], 'middle eastern': [], 'indian': []}\n",
      "asian               category\n",
      "white               category\n",
      "black               category\n",
      "other               category\n",
      "hispanic / latin    category\n",
      "pacific islander    category\n",
      "native american     category\n",
      "middle eastern      category\n",
      "indian              category\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "#breaking out ethnicity from a crowded column to a set of binary columns. In theory for easy processing later.\n",
    "ethnicity_df = pd.DataFrame(index=dating_data.index) \n",
    "pd.set_option('display.max_rows', 10)\n",
    "#print(dating_data['ethnicity'].value_counts())\n",
    "broken_out_count = {'nan': 0,}\n",
    "for ethnicity in dating_data['ethnicity']:\n",
    "    if type(ethnicity) == float:\n",
    "        broken_out_count['nan'] +=1\n",
    "    else:\n",
    "        if \",\" in ethnicity:\n",
    "            split_ethnic = ethnicity.split(',')\n",
    "            for splitted in split_ethnic:\n",
    "                stripped_split = splitted.strip()\n",
    "                if stripped_split not in broken_out_count:\n",
    "                    broken_out_count[stripped_split] = 0\n",
    "                broken_out_count[stripped_split] += 1\n",
    "        else:\n",
    "            if ethnicity not in broken_out_count:\n",
    "                broken_out_count[ethnicity] = 0\n",
    "            broken_out_count[ethnicity] += 1\n",
    "\n",
    "print(broken_out_count)\n",
    "#print(dating_data['ethnicity'].value_counts())\n",
    "\n",
    "ethnicity_set =set(broken_out_count.keys())\n",
    "print(ethnicity_set)\n",
    "ethnicity_columns = broken_out_count.copy()\n",
    "for key in ethnicity_columns.keys():\n",
    "    ethnicity_columns[key] = []\n",
    "print(ethnicity_columns)\n",
    "ethnicity_columns.pop('nan')\n",
    "\n",
    "for row in dating_data['ethnicity']:\n",
    "    if type(row) == float:\n",
    "        for key in ethnicity_columns.keys():\n",
    "            ethnicity_columns[key].append(0)\n",
    "    elif type(row) == str:\n",
    "        split_row = row.split(',')\n",
    "        \n",
    "        for index, item in enumerate(split_row):\n",
    "            split_row[index] = item.strip()\n",
    "        \n",
    "        for key in ethnicity_columns.keys():\n",
    "                if key in split_row:\n",
    "                    ethnicity_columns[key].append(1)\n",
    "                else:                    \n",
    "                    ethnicity_columns[key].append(0)\n",
    "\n",
    "for key, value in ethnicity_columns.items():\n",
    "    ethnicity_df[key] = value\n",
    "    ethnicity_df[key] = ethnicity_df[key].astype('category')\n",
    "    \n",
    "print(ethnicity_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "coastal-utility",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age     status sex orientation       body_type               diet  \\\n",
      "0   22     single   m    straight  a little extra  strictly anything   \n",
      "1   35     single   m    straight         average       mostly other   \n",
      "2   38  available   m    straight            thin           anything   \n",
      "3   23     single   m    straight            thin         vegetarian   \n",
      "4   29     single   m    straight        athletic                NaN   \n",
      "\n",
      "     drinks      drugs                          education  \\\n",
      "0  socially      never      working on college/university   \n",
      "1     often  sometimes              working on space camp   \n",
      "2  socially        NaN     graduated from masters program   \n",
      "3  socially        NaN      working on college/university   \n",
      "4  socially      never  graduated from college/university   \n",
      "\n",
      "             ethnicity  ...  income                          job  \\\n",
      "0         asian, white  ...      -1               transportation   \n",
      "1                white  ...   80000         hospitality / travel   \n",
      "2                  NaN  ...      -1                          NaN   \n",
      "3                white  ...   20000                      student   \n",
      "4  asian, black, other  ...      -1  artistic / musical / writer   \n",
      "\n",
      "          last_online                         location  \\\n",
      "0 2012-06-28 20:30:00  south san francisco, california   \n",
      "1 2012-06-29 21:41:00              oakland, california   \n",
      "2 2012-06-27 09:10:00        san francisco, california   \n",
      "3 2012-06-28 14:22:00             berkeley, california   \n",
      "4 2012-06-27 21:26:00        san francisco, california   \n",
      "\n",
      "                                offspring                       pets  \\\n",
      "0  doesn't have kids, but might want them  likes dogs and likes cats   \n",
      "1  doesn't have kids, but might want them  likes dogs and likes cats   \n",
      "2                                     NaN                   has cats   \n",
      "3                       doesn't want kids                 likes cats   \n",
      "4                                     NaN  likes dogs and likes cats   \n",
      "\n",
      "                                   religion  \\\n",
      "0     agnosticism and very serious about it   \n",
      "1  agnosticism but not too serious about it   \n",
      "2                                       NaN   \n",
      "3                                       NaN   \n",
      "4                                       NaN   \n",
      "\n",
      "                                 sign     smokes  \\\n",
      "0                              gemini  sometimes   \n",
      "1                              cancer         no   \n",
      "2  pisces but it doesn&rsquo;t matter         no   \n",
      "3                              pisces         no   \n",
      "4                            aquarius         no   \n",
      "\n",
      "                                              speaks  \n",
      "0                                            english  \n",
      "1  english (fluently), spanish (poorly), french (...  \n",
      "2                               english, french, c++  \n",
      "3                           english, german (poorly)  \n",
      "4                                            english  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "print(dating_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "above-geneva",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now that we've one hot encoded our ethnicity category we can get rid of it entirely\n",
    "dating_data.drop('ethnicity',axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "increasing-finding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'(okay)', '(poorly),', '(poorly)', '(fluently),', '(okay),', '(fluently)'}\n",
      "67\n",
      "['afrikaans,', 'arabic,', 'basque,', 'belarusan,', 'bengali,', 'breton,', 'bulgarian,', 'c++,', 'catalan,', 'cebuano,', 'chechen,', 'chinese,', 'croatian,', 'czech,', 'danish,', 'dutch,', 'english,', 'esperanto,', 'estonian,', 'farsi,', 'finnish,', 'french,', 'frisian,', 'georgian,', 'german,', 'greek,', 'gujarati,', 'hawaiian,', 'hebrew,', 'hindi,', 'hungarian,', 'ilongo,', 'indonesian,', 'irish,', 'italian,', 'japanese,', 'khmer,', 'korean,', 'language,', 'latin,', 'lisp,', 'malay,', 'maori,', 'norwegian,', 'occitan,', 'other,', 'persian,', 'polish,', 'portuguese,', 'romanian,', 'rotuman,', 'russian,', 'sanskrit,', 'serbian,', 'slovak,', 'spanish,', 'swahili,', 'swedish,', 'tagalog,', 'tamil,', 'thai,', 'tibetan,', 'turkish,', 'ukrainian,', 'urdu,', 'vietnamese,', 'yiddish,']\n",
      "77\n",
      "afrikaans\n",
      "albanian\n",
      "ancient greek\n",
      "arabic\n",
      "armenian\n",
      "basque\n",
      "belarusan\n",
      "bengali\n",
      "breton\n",
      "bulgarian\n",
      "c++\n",
      "catalan\n",
      "cebuano\n",
      "chechen\n",
      "chinese\n",
      "croatian\n",
      "czech\n",
      "danish\n",
      "dutch\n",
      "english\n",
      "esperanto\n",
      "estonian\n",
      "farsi\n",
      "finnish\n",
      "french\n",
      "frisian\n",
      "georgian\n",
      "german\n",
      "greek\n",
      "gujarati\n",
      "hawaiian\n",
      "hebrew\n",
      "hindi\n",
      "hungarian\n",
      "icelandic\n",
      "ilongo\n",
      "indonesian\n",
      "irish\n",
      "italian\n",
      "japanese\n",
      "khmer\n",
      "korean\n",
      "latin\n",
      "latvian\n",
      "lisp\n",
      "lithuanian\n",
      "malay\n",
      "maori\n",
      "mongolian\n",
      "norwegian\n",
      "occitan\n",
      "other\n",
      "persian\n",
      "polish\n",
      "portuguese\n",
      "romanian\n",
      "rotuman\n",
      "russian\n",
      "sanskrit\n",
      "sardinian\n",
      "serbian\n",
      "sign language\n",
      "slovak\n",
      "slovenian\n",
      "spanish\n",
      "swahili\n",
      "swedish\n",
      "tagalog\n",
      "tamil\n",
      "thai\n",
      "tibetan\n",
      "turkish\n",
      "ukrainian\n",
      "urdu\n",
      "vietnamese\n",
      "welsh\n",
      "yiddish\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 10)\n",
    "import numbers\n",
    "languages_set = set()\n",
    "\n",
    "#Build set of uniques\n",
    "\n",
    "for entry in dating_data['speaks'].values:\n",
    "    if isinstance(entry, numbers.Number) is False:\n",
    "        if \" \" in entry:\n",
    "            split_list = entry.split()\n",
    "            for word in split_list:\n",
    "                languages_set.add(word)\n",
    "        else:\n",
    "            languages_set.add(entry)\n",
    "            \n",
    "anti_language_set = set()\n",
    "fluency_set = set()\n",
    "#build set of redundancies\n",
    "for language in languages_set:\n",
    "    if language[0] == \"(\":\n",
    "        fluency_set.add(language) \n",
    "    if language[-1] == ',':\n",
    "        anti_language_set.add(language) \n",
    "\n",
    "#remove redundancies        \n",
    "for unlanguage in anti_language_set:\n",
    "    languages_set.remove(unlanguage)\n",
    "for fluency in fluency_set:\n",
    "    if fluency in languages_set:\n",
    "        languages_set.remove(fluency)    \n",
    "    if fluency in anti_language_set:        \n",
    "        anti_language_set.remove(fluency)\n",
    "\n",
    "languages_set.remove('language')\n",
    "languages_set.remove('ancient')\n",
    "languages_set.remove('sign')\n",
    "languages_set.add('ancient greek')\n",
    "languages_set.add('sign language')\n",
    "#display results        \n",
    "print(fluency_set)\n",
    "print(len(anti_language_set))\n",
    "#present redundant languages alphabetically\n",
    "ordered_unique_anti_languages = list(anti_language_set)\n",
    "ordered_unique_anti_languages =sorted(ordered_unique_anti_languages )\n",
    "print(ordered_unique_anti_languages )            \n",
    "print(len(languages_set))\n",
    "#present languages alphabetically\n",
    "ordered_unique_languages = list(languages_set)\n",
    "ordered_unique_languages = sorted(ordered_unique_languages)\n",
    "for language in ordered_unique_languages:\n",
    "    print(language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "consistent-mouth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "language,\n"
     ]
    }
   ],
   "source": [
    "#test to make sure our language list is complete. It is complete since this prints nothing\n",
    "for unlanguage in anti_language_set:\n",
    "    if unlanguage[:-1] not in languages_set:\n",
    "        print(unlanguage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "satisfied-conversation",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dating_data['speaks'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "involved-pickup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                              english\n",
      "1    english (fluently), spanish (poorly), french (...\n",
      "2                                 english, french, c++\n",
      "3                             english, german (poorly)\n",
      "4                                              english\n",
      "Name: speaks, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(dating_data['speaks'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "invalid-conditioning",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this was mostly so I could reference the data structure while I was working on the following section. Commented out because\n",
    "#when you view this on github you can't shrink the cell and it's absolutely gigantic.\n",
    "#for row in dating_data['speaks']:\n",
    "    #if type(row) != float:\n",
    "     #   print(row.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "isolated-sympathy",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "japanese      category\n",
      "tagalog       category\n",
      "maori         category\n",
      "arabic        category\n",
      "latin         category\n",
      "                ...   \n",
      "lithuanian    category\n",
      "danish        category\n",
      "spanish       category\n",
      "lisp          category\n",
      "norwegian     category\n",
      "Length: 77, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Here we're making language columns with numerical fluency values\n",
    "\n",
    "\n",
    "#making dictionary of lists to become columns\n",
    "language_columns = {}\n",
    "for unique in languages_set:\n",
    "    language_columns[unique] = []\n",
    "\n",
    "language_df = pd.DataFrame(index=dating_data.index)    \n",
    "    \n",
    "for row in dating_data['speaks']:\n",
    "    if type(row) != float:\n",
    "        split_row = row.split(',')\n",
    "        \n",
    "        for language in languages_set:\n",
    "            assignment_flag = 0\n",
    "            \n",
    "            for split_item in split_row:\n",
    "                if assignment_flag == 0:\n",
    "                    if language in split_item:\n",
    "                        if 'fluently' in split_item:\n",
    "                            language_columns[language].append(3)\n",
    "                            assignment_flag = 1\n",
    "                        elif 'okay' in split_item:\n",
    "                            language_columns[language].append(2)\n",
    "                            assignment_flag = 1\n",
    "                        elif 'poorly' in split_item:\n",
    "                            language_columns[language].append(1)\n",
    "                            assignment_flag = 1\n",
    "                        #we're assuming that if a fluency isn't specified they can be considered fluent. Domain knowledge suggests \n",
    "                        #non-specification indicates their native language.\n",
    "                        else:\n",
    "                            language_columns[language].append(3)\n",
    "                            assignment_flag = 1\n",
    "            if assignment_flag == 0:\n",
    "                language_columns[language].append(0)\n",
    "                        \n",
    "    else:\n",
    "        for language in languages_set:\n",
    "            language_columns[language].append(0)\n",
    "\n",
    "for key, value in language_columns.items():\n",
    "    language_df[key] = value\n",
    "    language_df[key] = language_df[key].astype('category')\n",
    "    \n",
    "print(language_df.dtypes)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "virgin-egypt",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now that we've one hot encoded our speaks column, we can get rid of it entirely.\n",
    "dating_data.drop('speaks',axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "practical-season",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "other                             7589\n",
      "student                           4882\n",
      "science / tech / engineering      4848\n",
      "computer / hardware / software    4709\n",
      "artistic / musical / writer       4439\n",
      "                                  ... \n",
      "rather not say                     436\n",
      "transportation                     366\n",
      "unemployed                         273\n",
      "retired                            250\n",
      "military                           204\n",
      "Name: job, Length: 21, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#checking out jobs\n",
    "print(dating_data['job'].value_counts())\n",
    "#This is obviously categories so we'll just convert it right over.\n",
    "dating_data['job'] = dating_data['job'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "sound-satisfaction",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('san francisco, california', 31064)\n",
      "('oakland, california', 7214)\n",
      "('berkeley, california', 4212)\n",
      "('san mateo, california', 1331)\n",
      "('palo alto, california', 1064)\n",
      "('alameda, california', 910)\n",
      "('san rafael, california', 755)\n",
      "('hayward, california', 747)\n",
      "('emeryville, california', 738)\n",
      "('redwood city, california', 693)\n",
      "('daly city, california', 681)\n",
      "('san leandro, california', 651)\n",
      "('walnut creek, california', 644)\n",
      "('vallejo, california', 558)\n",
      "('menlo park, california', 479)\n",
      "('richmond, california', 424)\n",
      "('south san francisco, california', 416)\n",
      "('mountain view, california', 384)\n",
      "('novato, california', 369)\n",
      "('burlingame, california', 361)\n",
      "('pleasant hill, california', 347)\n",
      "('castro valley, california', 345)\n",
      "('stanford, california', 341)\n",
      "('el cerrito, california', 325)\n",
      "('pacifica, california', 323)\n",
      "('martinez, california', 316)\n",
      "('mill valley, california', 315)\n",
      "('san bruno, california', 290)\n",
      "('san pablo, california', 245)\n",
      "('belmont, california', 243)\n",
      "('albany, california', 233)\n",
      "('san carlos, california', 227)\n",
      "('benicia, california', 203)\n",
      "('lafayette, california', 180)\n",
      "('sausalito, california', 179)\n",
      "('millbrae, california', 155)\n",
      "('san anselmo, california', 147)\n",
      "('el sobrante, california', 137)\n",
      "('san lorenzo, california', 135)\n",
      "('fairfax, california', 121)\n",
      "('hercules, california', 117)\n",
      "('pinole, california', 104)\n",
      "('half moon bay, california', 99)\n",
      "('fremont, california', 90)\n",
      "('green brae, california', 87)\n",
      "('orinda, california', 85)\n",
      "('moraga, california', 84)\n",
      "('larkspur, california', 80)\n",
      "('corte madera, california', 76)\n",
      "('belvedere tiburon, california', 57)\n",
      "('atherton, california', 45)\n",
      "('brisbane, california', 38)\n",
      "('rodeo, california', 37)\n",
      "('crockett, california', 32)\n",
      "('el granada, california', 27)\n",
      "('foster city, california', 24)\n",
      "('kentfield, california', 18)\n",
      "('woodacre, california', 16)\n",
      "('east palo alto, california', 13)\n",
      "('montara, california', 12)\n",
      "('ross, california', 12)\n",
      "('piedmont, california', 12)\n",
      "('woodside, california', 11)\n",
      "('westlake, california', 11)\n",
      "('los angeles, california', 10)\n",
      "('lagunitas, california', 10)\n",
      "('san geronimo, california', 9)\n",
      "('bolinas, california', 8)\n",
      "('point richmond, california', 8)\n",
      "('moss beach, california', 8)\n",
      "('west oakland, california', 7)\n",
      "('colma, california', 7)\n",
      "('san diego, california', 6)\n",
      "('santa cruz, california', 5)\n",
      "('tiburon, california', 5)\n",
      "('hillsborough, california', 4)\n",
      "('stinson beach, california', 4)\n",
      "('santa monica, california', 3)\n",
      "('bayshore, california', 3)\n",
      "('nicasio, california', 3)\n",
      "('san jose, california', 2)\n",
      "('petaluma, california', 2)\n",
      "('san quentin, california', 2)\n",
      "('sacramento, california', 2)\n",
      "('kensington, california', 2)\n",
      "('redwood shores, california', 2)\n",
      "('forest knolls, california', 2)\n",
      "('los gatos, california', 2)\n",
      "('santa rosa, california', 2)\n",
      "('irvine, california', 2)\n",
      "('napa, california', 2)\n",
      "('freedom, california', 1)\n",
      "('hacienda heights, california', 1)\n",
      "('riverside, california', 1)\n",
      "('rohnert park, california', 1)\n",
      "('canyon country, california', 1)\n",
      "('glencove, california', 1)\n",
      "('olema, california', 1)\n",
      "('union city, california', 1)\n",
      "('brea, california', 1)\n",
      "('santa clara, california', 1)\n",
      "('studio city, california', 1)\n",
      "('concord, california', 1)\n",
      "('seaside, california', 1)\n",
      "('magalia, california', 1)\n",
      "('orange, california', 1)\n",
      "('sunnyvale, california', 1)\n",
      "('ashland, california', 1)\n",
      "('pasadena, california', 1)\n",
      "('arcadia, california', 1)\n",
      "('milpitas, california', 1)\n",
      "('port costa, california', 1)\n",
      "('livingston, california', 1)\n",
      "('granite bay, california', 1)\n",
      "('isla vista, california', 1)\n",
      "('hilarita, california', 1)\n",
      "('campbell, california', 1)\n",
      "('santa ana, california', 1)\n",
      "('north hollywood, california', 1)\n",
      "('nevada city, california', 1)\n",
      "('stockton, california', 1)\n",
      "('marin city, california', 1)\n",
      "('waterford, california', 1)\n",
      "('muir beach, california', 1)\n",
      "('pacheco, california', 1)\n",
      "('canyon, california', 1)\n",
      "('oceanview, california', 1)\n",
      "('san luis obispo, california', 1)\n",
      "('modesto, california', 1)\n",
      "('costa mesa, california', 1)\n",
      "('oakley, california', 1)\n",
      "('chico, california', 1)\n",
      "('south lake tahoe, california', 1)\n",
      "('vacaville, california', 1)\n",
      "('long beach, california', 1)\n"
     ]
    }
   ],
   "source": [
    "#exploring location. Unsure if we want to turn this into categories or not.\n",
    "import collections\n",
    "#print(dating_data['location'].value_counts())\n",
    "location_counter_dict = collections.OrderedDict()\n",
    "for each in dating_data['location']:\n",
    "    if 'california' in each:\n",
    "        if each not in location_counter_dict:\n",
    "            location_counter_dict[each] = 0\n",
    "        location_counter_dict[each] += 1\n",
    "        \n",
    "sorted_california_locations = sorted(location_counter_dict.items(), key=lambda x: x[1], reverse= True)\n",
    "for each in sorted_california_locations:\n",
    "    print(each)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "legendary-muscle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('california', 59855), ('new york', 17), ('illinois', 8), ('massachusetts', 5), ('oregon', 4), ('michigan', 4), ('texas', 4), ('arizona', 3), ('florida', 3), ('colorado', 2), ('hawaii', 2), ('virginia', 2), ('spain', 2), ('united kingdom', 2), ('minnesota', 2), ('georgia', 2), ('utah', 2), ('washington', 2), ('district of columbia', 2), ('ohio', 2), ('montana', 1), ('wisconsin', 1), ('nevada', 1), ('vietnam', 1), ('ireland', 1), ('louisiana', 1), ('north carolina', 1), ('idaho', 1), ('mississippi', 1), ('new jersey', 1), ('west virginia', 1), ('connecticut', 1), ('tennessee', 1), ('rhode island', 1), ('british columbia', 1), ('missouri', 1), ('germany', 1), ('pennsylvania', 1), ('netherlands', 1), ('switzerland', 1), ('mexico', 1)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "count_dict = collections.OrderedDict()\n",
    "for row in dating_data['location']:\n",
    "    split_row = row.split(',')\n",
    "    stripped = split_row[1].strip()\n",
    "    if stripped not in count_dict:\n",
    "        count_dict[stripped] = 0\n",
    "    count_dict[stripped] += 1\n",
    "sorted_counts = sorted(count_dict.items(), key=lambda x: x[1], reverse= True)\n",
    "print(sorted_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "engaging-bedroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above is showing me that the city parameter is only really relevant within california... buuut most of my data\n",
    "# exists within california\n",
    "\n",
    "#Decided this isn't an interesting predictor, chopping it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "warming-jason",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dating_data.drop('location', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "mobile-format",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age               int64\n",
      "status           object\n",
      "sex            category\n",
      "orientation    category\n",
      "body_type      category\n",
      "                 ...   \n",
      "offspring        object\n",
      "pets           category\n",
      "religion         object\n",
      "sign           category\n",
      "smokes         category\n",
      "Length: 18, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(dating_data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "demographic-period",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#This was useful to set up to deal with the NaN values but it's unnecessary clutter for the external viewer. Leaving it in as \n",
    "#comments for posterities sake though.\n",
    "\n",
    "#for each in dating_data.columns:\n",
    "#    nantotal = dating_data[each].isna().sum()\n",
    "#    if nantotal > 0:\n",
    "#        print(each, nantotal)\n",
    "#        print(dating_data[each].unique())\n",
    "#        print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "academic-treasure",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bodytype 5296\n",
      "bodytype 0\n"
     ]
    }
   ],
   "source": [
    "#Dealing with body type NaNs. There's already a \"rather not say\" category. I believe that a NaN value indicates that they just \n",
    "#didn't answer the question, which I'm comfortable imputing as 'rather not say', since they chose not to say and body type is \n",
    "#a question asked early on in the profile creation process.\n",
    "print('bodytype', dating_data['body_type'].isna().sum())\n",
    "dating_data['body_type'].fillna('rather not say', inplace=True)\n",
    "print('bodytype', dating_data['body_type'].isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "linear-turkey",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diet 24395\n",
      "diet 0\n"
     ]
    }
   ],
   "source": [
    "#Dealing with diet next. #Here I'm going to assume that if you didn't specify your diet it wasn't important to you. Which means \n",
    "# you're probably operating under the default american diet (most of our data comes from california), which means anything.\n",
    "print('diet', dating_data['diet'].isna().sum())    \n",
    "dating_data['diet'].fillna('anything', inplace=True)\n",
    "print('diet', dating_data['diet'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bacterial-amino",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drinks 2985\n",
      "drinks 0\n"
     ]
    }
   ],
   "source": [
    "#Next we're goign to look at drinks Here it's a little less clear what to impute. Stances on drinking can vary pretty wildly. \n",
    "#I'm going to take a safe position and make a new category 'unspecified' and call it good. Not answering the question might\n",
    "#be a predictive behavior, who knows?\n",
    "print('drinks', dating_data['drinks'].isna().sum())\n",
    "dating_data['drinks'].cat.add_categories('unspecified', inplace=True)\n",
    "dating_data['drinks'].fillna('unspecified', inplace=True)\n",
    "print('drinks', dating_data['drinks'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ambient-flashing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drugs 14080\n",
      "drugs 0\n"
     ]
    }
   ],
   "source": [
    "#drugs next, here we're going to take the same approach as with drinking. Not specifying is it's own choice.\n",
    "print('drugs', dating_data['drugs'].isna().sum())\n",
    "dating_data['drugs'].cat.add_categories('unspecified', inplace=True)\n",
    "dating_data['drugs'].fillna('unspecified', inplace=True)\n",
    "print('drugs', dating_data['drugs'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "continent-pixel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "education 6628\n",
      "education 0\n"
     ]
    }
   ],
   "source": [
    "#and now to education. We're going to default to unspecificed again. Education is important to a lot of people, not specifying \n",
    "#it is a choice, and it very well could be predictive.\n",
    "print('education', dating_data['education'].isna().sum())\n",
    "dating_data['education'].cat.add_categories('unspecified', inplace=True)\n",
    "dating_data['education'].fillna('unspecified', inplace=True)\n",
    "print('education', dating_data['education'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "invisible-click",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(dating_data.education.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "nasty-contractor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "height 3\n",
      "height 0\n"
     ]
    }
   ],
   "source": [
    "#next up is height. I'm going to drop the rows with NaN here, there's only three of them\n",
    "print('height', dating_data['height'].isna().sum())\n",
    "dating_data.dropna(subset=['height'], inplace=True)\n",
    "print('height', dating_data['height'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "exact-camel",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-50-1ef6fbdcfc8d>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dating_data.height[mask] = dating_data.height.mean()\n"
     ]
    }
   ],
   "source": [
    "mask = dating_data.height < 24\n",
    "dating_data.height[mask] = dating_data.height.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "warming-writing",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "distinguished-estonia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job 8196\n",
      "job 0\n"
     ]
    }
   ],
   "source": [
    "#Jobs are up to bat! We're going to take advantage of the preexisting 'rather not say' category here. Again, employment is \n",
    "#important to most americans, choosing not to specify it is a noteworthy choice on a dating profile.\n",
    "print('job', dating_data['job'].isna().sum())\n",
    "dating_data['job'].fillna('rather not say', inplace=True)\n",
    "print('job', dating_data['job'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "institutional-steering",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dating_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-fc2757cd9154>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# I defienitely can't drop rows with NaN here and I'm not quite ready to drop the column entirely, so we're going to go with\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# our trusty 'unspecified' category\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'offspring'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdating_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'offspring'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"that's over half of our rows, I think it makes sense to drop this column entirely\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mdating_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'offspring'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dating_data' is not defined"
     ]
    }
   ],
   "source": [
    "# offspring now # we have a ton of NaNs here. Enough that it makes me skeptical of this column being predictive. \n",
    "# I defienitely can't drop rows with NaN here and I'm not quite ready to drop the column entirely, so we're going to go with \n",
    "# our trusty 'unspecified' category\n",
    "print('offspring', dating_data['offspring'].isna().sum())\n",
    "print(\"that's over half of our rows, I think it makes sense to drop this column entirely\")\n",
    "dating_data.drop('offspring', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "brutal-concert",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pets 19919\n",
      "pets 0\n"
     ]
    }
   ],
   "source": [
    "#pets. Same problem and logic as offspring. Too many to just drop, and even though there's enough NaNs to make me question the \n",
    "#predictive power of the column as a whole I don't want to drop it wholesale, so 'unspecified' yet again\n",
    "print('pets', dating_data['pets'].isna().sum())\n",
    "dating_data['pets'].cat.add_categories('unspecified', inplace=True)\n",
    "dating_data['pets'].fillna('unspecified', inplace=True)\n",
    "print('pets', dating_data['pets'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "threatened-vector",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "religion 20223\n",
      "religion 0\n"
     ]
    }
   ],
   "source": [
    "# religion. Same logic/problem as before. There's a preexisting other category, but I think that's significantly different from \n",
    "# not specifiying, so we're going to throw in unspecified again\n",
    "print('religion', dating_data['religion'].isna().sum())\n",
    "dating_data['religion'].fillna('unspecified', inplace=True)\n",
    "print('religion', dating_data['religion'].isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "chief-display",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59943, 18)\n",
      "sign 11054\n",
      "sign 0\n",
      "(59943, 18)\n"
     ]
    }
   ],
   "source": [
    "#smokes. This is the variable we're trying to predict. Makes \n",
    "print(dating_data.shape)\n",
    "print('sign', dating_data['sign'].isna().sum())\n",
    "dating_data['sign'].cat.add_categories('unspecified', inplace=True)\n",
    "dating_data['sign'].fillna('unspecified', inplace=True)\n",
    "print('sign', dating_data['sign'].isna().sum())\n",
    "print(dating_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "known-nomination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['scorpio and it matters a lot', 'libra and it matters a lot', 'leo and it matters a lot', 'cancer and it matters a lot', 'pisces and it matters a lot', 'aries and it matters a lot', 'capricorn and it matters a lot', 'aquarius and it matters a lot', 'sagittarius and it matters a lot', 'gemini and it matters a lot', 'taurus and it matters a lot', 'virgo and it matters a lot']\n",
      "SPACER\n",
      "['pisces but it doesn&rsquo;t matter', 'gemini but it doesn&rsquo;t matter', 'cancer but it doesn&rsquo;t matter', 'leo but it doesn&rsquo;t matter', 'aquarius but it doesn&rsquo;t matter', 'libra but it doesn&rsquo;t matter', 'taurus but it doesn&rsquo;t matter', 'sagittarius but it doesn&rsquo;t matter', 'virgo but it doesn&rsquo;t matter', 'capricorn but it doesn&rsquo;t matter', 'aries but it doesn&rsquo;t matter', 'scorpio but it doesn&rsquo;t matter']\n",
      "SPACER\n",
      "['aries and it&rsquo;s fun to think about', 'pisces and it&rsquo;s fun to think about', 'gemini and it&rsquo;s fun to think about', 'leo and it&rsquo;s fun to think about', 'cancer and it&rsquo;s fun to think about', 'libra and it&rsquo;s fun to think about', 'aquarius and it&rsquo;s fun to think about', 'scorpio and it&rsquo;s fun to think about', 'capricorn and it&rsquo;s fun to think about', 'sagittarius and it&rsquo;s fun to think about', 'taurus and it&rsquo;s fun to think about', 'virgo and it&rsquo;s fun to think about']\n",
      "SPACER\n",
      "['gemini', 'cancer', 'pisces', 'aquarius', 'taurus', 'virgo', 'sagittarius', 'unspecified', 'libra', 'scorpio', 'leo', 'aries', 'capricorn']\n",
      "SPACER\n"
     ]
    }
   ],
   "source": [
    "matter = []\n",
    "doesnot = []\n",
    "fun = []\n",
    "remainder = []\n",
    "for unique in dating_data.sign.unique():\n",
    "    if unique.find('lot') != -1:\n",
    "        matter.append(unique)\n",
    "    elif unique.find('but') != -1:    \n",
    "        doesnot.append(unique)        \n",
    "    elif unique.find('fun') != -1:    \n",
    "        fun.append(unique)\n",
    "    else:\n",
    "        remainder.append(unique)\n",
    "\n",
    "    \n",
    "print(matter)\n",
    "print('SPACER')\n",
    "print(doesnot)\n",
    "print('SPACER')\n",
    "print(fun)\n",
    "print('SPACER')\n",
    "print(remainder)\n",
    "print('SPACER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "least-porter",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.sign.value_counts()\n",
    "\n",
    "#Okay, similar to religion. 12 categories, plus other and each category has 4 levels. Every cat/level combo has at least 700 members with the exception of the \"Matters a lot\" category. \n",
    "# the most populated being doesn'Doesn't Matter' and 'Fun to think about'. I'm going to collapse this into four categories based on intensity of feeling, and one for unspecified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "guilty-prague",
   "metadata": {},
   "outputs": [],
   "source": [
    "dating_data.sign.replace(to_replace=matter, value='Cares a lot', inplace=True)\n",
    "dating_data.sign.replace(to_replace=doesnot, value='does not care', inplace=True)\n",
    "dating_data.sign.replace(to_replace=fun, value='fun to think about', inplace=True)\n",
    "dating_data.sign.replace(to_replace=remainder, value='just specified', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "voluntary-chapel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "just specified        23177\n",
       "fun to think about    19333\n",
       "does not care         16758\n",
       "Cares a lot             675\n",
       "Name: sign, dtype: int64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dating_data.sign.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "reasonable-manitoba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59943, 18)\n",
      "smokes 5510\n",
      "smokes 0\n",
      "(54433, 18)\n"
     ]
    }
   ],
   "source": [
    "#smokes. This is the variable we're trying to predict. Makes \n",
    "print(dating_data.shape)\n",
    "print('smokes', dating_data['smokes'].isna().sum())\n",
    "dating_data.dropna(subset=['smokes'], inplace=True)\n",
    "print('smokes', dating_data['smokes'].isna().sum())\n",
    "print(dating_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "continuing-episode",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decided that the last online column won't be generalizable so we're going to drop it wholseale\n",
    "dating_data = dating_data.drop('last_online', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "regional-assets",
   "metadata": {},
   "outputs": [],
   "source": [
    "#it became evident that most of the income data is missing (-1) and without a way to impute that, I'm going to drop the column \n",
    "#wholesale.\n",
    "\n",
    "dating_data= dating_data.drop('income', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "temporal-installation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "    \n",
    "    \n",
    "# This utility was created because students were getting confused when they ran \n",
    "# their notebooks twice, the previous write-to-file code would do nothing and say  \n",
    "# nothing. The students thought the file was over-written when in fact, it was not -\n",
    "# generating hidden bugs in subsequent notebooks.\n",
    "\n",
    "def save_file(data, fname, dname):\n",
    "    \"\"\"Save a datafile (data) to a specific location (dname) and filename (fname)\n",
    "    \n",
    "    Currently valid formats are limited to CSV or PKL.\"\"\"\n",
    "    \n",
    "    if not os.path.exists(dname):\n",
    "        os.mkdir(dname)\n",
    "        print(f'Directory {dname} was created.')\n",
    "        \n",
    "    fpath = os.path.join(dname, fname)\n",
    "    \n",
    "    \n",
    "    if os.path.exists(fpath):\n",
    "        print(\"A file already exists with this name.\\n\")\n",
    "\n",
    "        yesno = None\n",
    "        while yesno != \"Y\" and yesno != \"N\":\n",
    "            yesno = input('Do you want to overwrite? (Y/N)').strip()[0].capitalize()\n",
    "            if yesno == \"Y\":\n",
    "                print(f'Writing file.  \"{fpath}\"')\n",
    "                _save_file(data, fpath)\n",
    "                break  # Not required\n",
    "            elif yesno == \"N\":\n",
    "                print('\\nPlease re-run this cell with a new filename.')\n",
    "                break  # Not required\n",
    "            else:\n",
    "                print('\\nUnknown input, please enter \"Y\" or \"N\".')\n",
    "\n",
    "    else:  # path does not exist, ok to save the file\n",
    "        print(f'Writing file.  \"{fpath}\"')\n",
    "        _save_file(data, fpath)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "def _save_file(data, fpath):\n",
    "    valid_ftypes = ['.csv', '.pkl']\n",
    "    \n",
    "    assert (fpath[-4:] in valid_ftypes), \"Invalid file type.  Use '.csv' or '.pkl'\"\n",
    "\n",
    "    # Figure out what kind of file we're dealing with by name\n",
    "    if fpath[-3:] == 'csv':\n",
    "        data.to_csv(fpath, index=False)\n",
    "    elif fpath[-3:] == 'pkl':\n",
    "        with open(fpath, 'wb') as f:\n",
    "            pickle.dump(data, f)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "empty-breed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A file already exists with this name.\n",
      "\n",
      "\n",
      "Please re-run this cell with a new filename.\n"
     ]
    }
   ],
   "source": [
    "save_file(dating_data, 'wrangled_dating_data.csv', 'derived_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "deadly-positive",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A file already exists with this name.\n",
      "\n",
      "Writing file.  \"derived_data\\wrangled_language_data.csv\"\n"
     ]
    }
   ],
   "source": [
    "save_file(language_df, 'wrangled_language_data.csv', 'derived_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "super-dubai",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A file already exists with this name.\n",
      "\n",
      "\n",
      "Please re-run this cell with a new filename.\n"
     ]
    }
   ],
   "source": [
    "save_file(ethnicity_df, 'wrangled_ethnicity_data.csv', 'derived_data')\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "53a481214c1eee3a60e2d0cf4c920992715a8f8087f54faa7a39dc4a3c3b8bd0"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
