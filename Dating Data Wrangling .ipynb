{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "forty-filename",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "applied-workshop",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Collection\n",
    "#Goal: Organize your data to streamline the next steps of your\n",
    "#capstone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cellular-element",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data source:\n",
      "    https://www.kaggle.com/andrewmvd/okcupid-profiles\n"
     ]
    }
   ],
   "source": [
    "print('''The data source:\n",
    "    https://www.kaggle.com/andrewmvd/okcupid-profiles''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dynamic-uncertainty",
   "metadata": {},
   "outputs": [],
   "source": [
    "dating_data = pd.read_csv('okcupid_profiles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "nonprofit-guatemala",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age  status sex orientation       body_type               diet    drinks  \\\n",
      "0   22  single   m    straight  a little extra  strictly anything  socially   \n",
      "1   35  single   m    straight         average       mostly other     often   \n",
      "\n",
      "       drugs                      education     ethnicity  ...  \\\n",
      "0      never  working on college/university  asian, white  ...   \n",
      "1  sometimes          working on space camp         white  ...   \n",
      "\n",
      "                                              essay0  \\\n",
      "0  about me:  i would love to think that i was so...   \n",
      "1  i am a chef: this is what that means. 1. i am ...   \n",
      "\n",
      "                                              essay1  \\\n",
      "0  currently working as an international agent fo...   \n",
      "1  dedicating everyday to being an unbelievable b...   \n",
      "\n",
      "                                              essay2  \\\n",
      "0  making people laugh. ranting about a good salt...   \n",
      "1  being silly. having ridiculous amonts of fun w...   \n",
      "\n",
      "                                              essay3  \\\n",
      "0  the way i look. i am a six foot half asian, ha...   \n",
      "1                                                NaN   \n",
      "\n",
      "                                              essay4  \\\n",
      "0  books: absurdistan, the republic, of mice and ...   \n",
      "1  i am die hard christopher moore fan. i don't r...   \n",
      "\n",
      "                                              essay5  \\\n",
      "0                  food. water. cell phone. shelter.   \n",
      "1  delicious porkness in all of its glories. my b...   \n",
      "\n",
      "                        essay6  \\\n",
      "0  duality and humorous things   \n",
      "1                          NaN   \n",
      "\n",
      "                                              essay7  \\\n",
      "0  trying to find someone to hang out with. i am ...   \n",
      "1                                                NaN   \n",
      "\n",
      "                                              essay8  \\\n",
      "0  i am new to california and looking for someone...   \n",
      "1  i am very open and will share just about anyth...   \n",
      "\n",
      "                                              essay9  \n",
      "0  you want to be swept off your feet! you are ti...  \n",
      "1                                                NaN  \n",
      "\n",
      "[2 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "print(dating_data.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "proof-result",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Organization\n",
    "# Goal: Create a file structure and add your work to the GitHub\n",
    "#repository youâ€™ve created for this project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adverse-butterfly",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The repo is here\n",
      "https://github.com/Prathmun/DatingDataCapstone\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('''The repo is here\n",
    "https://github.com/Prathmun/DatingDataCapstone\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "rubber-costume",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age: is a: <class 'numpy.int64'> and it has *54* unique values\n",
      "status: is a: <class 'str'> and it has *5* unique values\n",
      "sex: is a: <class 'str'> and it has *2* unique values\n",
      "orientation: is a: <class 'str'> and it has *3* unique values\n",
      "body_type: is a: <class 'str'> and it has *13* unique values\n",
      "diet: is a: <class 'str'> and it has *19* unique values\n",
      "drinks: is a: <class 'str'> and it has *7* unique values\n",
      "drugs: is a: <class 'float'> and it has *4* unique values\n",
      "education: is a: <class 'str'> and it has *33* unique values\n",
      "ethnicity: is a: <class 'float'> and it has *218* unique values\n",
      "height: is a: <class 'numpy.float64'> and it has *61* unique values\n",
      "income: is a: <class 'numpy.int64'> and it has *13* unique values\n",
      "job: is a: <class 'float'> and it has *22* unique values\n",
      "last_online: is a: <class 'str'> and it has *30123* unique values\n",
      "location: is a: <class 'str'> and it has *199* unique values\n",
      "offspring: is a: <class 'float'> and it has *16* unique values\n",
      "pets: is a: <class 'str'> and it has *16* unique values\n",
      "religion: is a: <class 'float'> and it has *46* unique values\n",
      "sign: is a: <class 'str'> and it has *49* unique values\n",
      "smokes: is a: <class 'str'> and it has *6* unique values\n",
      "speaks: is a: <class 'str'> and it has *7648* unique values\n"
     ]
    }
   ],
   "source": [
    "for header in dating_data.columns:\n",
    "    if 'essay' not in header:\n",
    "        print(header + \": is a: \" + str(type(dating_data[header].values[2])) + \" and it has *\" + str(len(pd.unique(dating_data[header])))\n",
    "             + \"* unique values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "exempt-shelter",
   "metadata": {},
   "outputs": [],
   "source": [
    "#don't need those essay columns for this project\n",
    "droplist = []\n",
    "for i in range(10):\n",
    "    appendable = 'essay'+str(i)\n",
    "    droplist.append(appendable)\n",
    "dating_data = dating_data.drop(droplist, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "smooth-mineral",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age              int64\n",
       "status          object\n",
       "sex             object\n",
       "orientation     object\n",
       "body_type       object\n",
       "diet            object\n",
       "drinks          object\n",
       "drugs           object\n",
       "education       object\n",
       "ethnicity       object\n",
       "height         float64\n",
       "income           int64\n",
       "job             object\n",
       "last_online     object\n",
       "location        object\n",
       "offspring       object\n",
       "pets            object\n",
       "religion        object\n",
       "sign            object\n",
       "smokes          object\n",
       "speaks          object\n",
       "dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dating_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "moral-election",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doesn't have kids, but might want them\n",
      "doesn't have kids, but might want them\n",
      "nan\n",
      "doesn't want kids\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "#What kind of data is this, are these categories?\n",
    "for i in range(5):\n",
    "    print(dating_data['offspring'].values[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "exterior-disease",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Yep. Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "floating-domain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "likes dogs and likes cats\n",
      "likes dogs and likes cats\n",
      "has cats\n",
      "likes cats\n",
      "likes dogs and likes cats\n"
     ]
    }
   ],
   "source": [
    "#What kind of data is this, are these categories?\n",
    "for i in range(5):\n",
    "    print(dating_data['pets'].values[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "turkish-wellington",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Also yep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "serial-province",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agnosticism and very serious about it\n",
      "agnosticism but not too serious about it\n",
      "nan\n",
      "nan\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "#What kind of data is this, are these categories?\n",
    "for i in range(5):\n",
    "    print(dating_data['religion'].values[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "charitable-kennedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#also also yep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "scheduled-signal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gemini\n",
      "cancer\n",
      "pisces but it doesn&rsquo;t matter\n",
      "pisces\n",
      "aquarius\n"
     ]
    }
   ],
   "source": [
    "#What kind of data is this, are these categories?\n",
    "for i in range(5):\n",
    "    print(dating_data['sign'].values[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "hollow-planning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gemini and it&rsquo;s fun to think about         1782\n",
      "scorpio and it&rsquo;s fun to think about        1772\n",
      "leo and it&rsquo;s fun to think about            1692\n",
      "libra and it&rsquo;s fun to think about          1649\n",
      "taurus and it&rsquo;s fun to think about         1640\n",
      "cancer and it&rsquo;s fun to think about         1597\n",
      "pisces and it&rsquo;s fun to think about         1592\n",
      "sagittarius and it&rsquo;s fun to think about    1583\n",
      "virgo and it&rsquo;s fun to think about          1574\n",
      "aries and it&rsquo;s fun to think about          1573\n",
      "aquarius and it&rsquo;s fun to think about       1503\n",
      "virgo but it doesn&rsquo;t matter                1497\n",
      "leo but it doesn&rsquo;t matter                  1457\n",
      "cancer but it doesn&rsquo;t matter               1454\n",
      "gemini but it doesn&rsquo;t matter               1453\n",
      "taurus but it doesn&rsquo;t matter               1450\n",
      "aquarius but it doesn&rsquo;t matter             1408\n",
      "libra but it doesn&rsquo;t matter                1408\n",
      "capricorn and it&rsquo;s fun to think about      1376\n",
      "sagittarius but it doesn&rsquo;t matter          1375\n",
      "aries but it doesn&rsquo;t matter                1373\n",
      "capricorn but it doesn&rsquo;t matter            1319\n",
      "pisces but it doesn&rsquo;t matter               1300\n",
      "scorpio but it doesn&rsquo;t matter              1264\n",
      "leo                                              1159\n",
      "libra                                            1098\n",
      "cancer                                           1092\n",
      "virgo                                            1029\n",
      "scorpio                                          1020\n",
      "gemini                                           1013\n",
      "taurus                                           1001\n",
      "aries                                             996\n",
      "pisces                                            992\n",
      "aquarius                                          954\n",
      "sagittarius                                       937\n",
      "capricorn                                         833\n",
      "scorpio and it matters a lot                       78\n",
      "leo and it matters a lot                           66\n",
      "cancer and it matters a lot                        63\n",
      "aquarius and it matters a lot                      63\n",
      "gemini and it matters a lot                        62\n",
      "pisces and it matters a lot                        62\n",
      "libra and it matters a lot                         52\n",
      "taurus and it matters a lot                        49\n",
      "sagittarius and it matters a lot                   47\n",
      "aries and it matters a lot                         47\n",
      "capricorn and it matters a lot                     45\n",
      "virgo and it matters a lot                         41\n",
      "Name: sign, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(dating_data['sign'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fiscal-density",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Yep, thankfully more cats not freeform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "hollywood-running",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english\n",
      "english (fluently), spanish (poorly), french (poorly)\n",
      "english, french, c++\n",
      "english, german (poorly)\n",
      "english\n",
      "7648\n"
     ]
    }
   ],
   "source": [
    "#What kind of data is this, are these categories?\n",
    "for i in range(5):\n",
    "    print(dating_data['speaks'].values[i])\n",
    "print(len(pd.unique(dating_data['speaks'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "metropolitan-compound",
   "metadata": {},
   "outputs": [],
   "source": [
    "# looks like cats because spellings are uniform, but more exploration required because the c++ is throwing me off. \n",
    "# Can probably be converted into categories though. There are only so many languages. \n",
    "#The unique count is huge, but with the combinations of possible languages and fluency, I'm still not convinced it can't be a \n",
    "#kind of categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "sustainable-stupid",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   2012-06-28 20:30:00\n",
      "1   2012-06-29 21:41:00\n",
      "2   2012-06-27 09:10:00\n",
      "3   2012-06-28 14:22:00\n",
      "4   2012-06-27 21:26:00\n",
      "Name: last_online, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "print(dating_data['last_online'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "developmental-leadership",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       2012-06-28 20:30:00\n",
      "1       2012-06-29 21:41:00\n",
      "2       2012-06-27 09:10:00\n",
      "3       2012-06-28 14:22:00\n",
      "4       2012-06-27 21:26:00\n",
      "                ...        \n",
      "59941   2012-06-12 21:47:00\n",
      "59942   2012-06-29 11:01:00\n",
      "59943   2012-06-27 23:37:00\n",
      "59944   2012-06-23 13:01:00\n",
      "59945   2012-06-29 00:42:00\n",
      "Name: last_online, Length: 59946, dtype: datetime64[ns]\n",
      "<class 'numpy.datetime64'>\n"
     ]
    }
   ],
   "source": [
    "#converting last online from strings to datetime objects. I kind of doubt we're going to use this column, but it seemed \n",
    "#tidy to convert it.\n",
    "dating_data['last_online'] = pd.to_datetime(dating_data['last_online'], format='%Y-%m-%d-%H-%M')\n",
    "print(dating_data['last_online'])\n",
    "print(type(dating_data['last_online'].values[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "atmospheric-crisis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m    35829\n",
      "f    24117\n",
      "Name: sex, dtype: int64\n",
      "0        m\n",
      "1        m\n",
      "2        m\n",
      "3        m\n",
      "4        m\n",
      "        ..\n",
      "59941    f\n",
      "59942    m\n",
      "59943    m\n",
      "59944    m\n",
      "59945    m\n",
      "Name: sex, Length: 59946, dtype: category\n",
      "Categories (2, object): ['f', 'm']\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "#exploring turning a simple column into categories. IE: sex\n",
    "\n",
    "print(dating_data['sex'].value_counts())\n",
    "dating_data['sex'] = dating_data['sex'].astype('category')\n",
    "\n",
    "print(dating_data['sex'])\n",
    "print(type(dating_data['sex'].values[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "heated-belize",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the cleaner string columns into categories now that we've tested it. Leaving out Ethnicity, location \n",
    "#and speaks for now as they appear more complicated. We'll fiddle with them next. Jobs as well is a little unclear, we'll \n",
    "#explore that shortly as well.\n",
    "\n",
    "dating_data['orientation'] = dating_data['orientation'].astype('category')\n",
    "dating_data['body_type'] = dating_data['body_type'].astype('category')\n",
    "dating_data['diet'] = dating_data['diet'].astype('category')\n",
    "dating_data['drinks'] = dating_data['drinks'].astype('category')\n",
    "dating_data['drugs'] = dating_data['drugs'].astype('category')\n",
    "dating_data['education'] = dating_data['education'].astype('category')\n",
    "dating_data['location'] = dating_data['location'].astype('category')\n",
    "dating_data['offspring'] = dating_data['offspring'].astype('category')\n",
    "dating_data['pets'] = dating_data['pets'].astype('category')\n",
    "dating_data['religion'] = dating_data['religion'].astype('category')\n",
    "dating_data['sign'] = dating_data['sign'].astype('category')\n",
    "dating_data['smokes'] = dating_data['smokes'].astype('category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "opposite-south",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                     int64\n",
       "status                 object\n",
       "sex                  category\n",
       "orientation          category\n",
       "body_type            category\n",
       "diet                 category\n",
       "drinks               category\n",
       "drugs                category\n",
       "education            category\n",
       "ethnicity              object\n",
       "height                float64\n",
       "income                  int64\n",
       "job                    object\n",
       "last_online    datetime64[ns]\n",
       "location             category\n",
       "offspring            category\n",
       "pets                 category\n",
       "religion             category\n",
       "sign                 category\n",
       "smokes               category\n",
       "speaks                 object\n",
       "dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#just checking to see if the above worked\n",
    "dating_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "going-variation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59946, 21)\n"
     ]
    }
   ],
   "source": [
    "#This was the first place I thought to check this, used it a lot in testing and constructing later functions\n",
    "print(dating_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ethical-biotechnology",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'nan': 5680, 'asian': 8205, 'white': 37882, 'black': 3328, 'other': 3567, 'hispanic / latin': 5357, 'pacific islander': 1473, 'native american': 1265, 'middle eastern': 950, 'indian': 1449}\n",
      "{'other', 'black', 'native american', 'indian', 'middle eastern', 'asian', 'white', 'hispanic / latin', 'pacific islander', 'nan'}\n",
      "{'nan': [], 'asian': [], 'white': [], 'black': [], 'other': [], 'hispanic / latin': [], 'pacific islander': [], 'native american': [], 'middle eastern': [], 'indian': []}\n",
      "age                    int64\n",
      "status                object\n",
      "sex                 category\n",
      "orientation         category\n",
      "body_type           category\n",
      "                      ...   \n",
      "hispanic / latin    category\n",
      "pacific islander    category\n",
      "native american     category\n",
      "middle eastern      category\n",
      "indian              category\n",
      "Length: 30, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#breaking out ethnicity from a crowded column to a set of binary columns. In theory for easy processing later.\n",
    "\n",
    "pd.set_option('display.max_rows', 10)\n",
    "#print(dating_data['ethnicity'].value_counts())\n",
    "broken_out_count = {'nan': 0,}\n",
    "for ethnicity in dating_data['ethnicity']:\n",
    "    if type(ethnicity) == float:\n",
    "        broken_out_count['nan'] +=1\n",
    "    else:\n",
    "        if \",\" in ethnicity:\n",
    "            split_ethnic = ethnicity.split(',')\n",
    "            for splitted in split_ethnic:\n",
    "                stripped_split = splitted.strip()\n",
    "                if stripped_split not in broken_out_count:\n",
    "                    broken_out_count[stripped_split] = 0\n",
    "                broken_out_count[stripped_split] += 1\n",
    "        else:\n",
    "            if ethnicity not in broken_out_count:\n",
    "                broken_out_count[ethnicity] = 0\n",
    "            broken_out_count[ethnicity] += 1\n",
    "\n",
    "print(broken_out_count)\n",
    "#print(dating_data['ethnicity'].value_counts())\n",
    "\n",
    "ethnicity_set =set(broken_out_count.keys())\n",
    "print(ethnicity_set)\n",
    "ethnicity_columns = broken_out_count.copy()\n",
    "for key in ethnicity_columns.keys():\n",
    "    ethnicity_columns[key] = []\n",
    "print(ethnicity_columns)\n",
    "ethnicity_columns.pop('nan')\n",
    "\n",
    "for row in dating_data['ethnicity']:\n",
    "    if type(row) == float:\n",
    "        for key in ethnicity_columns.keys():\n",
    "            ethnicity_columns[key].append(0)\n",
    "    elif type(row) == str:\n",
    "        split_row = row.split(',')\n",
    "        \n",
    "        for index, item in enumerate(split_row):\n",
    "            split_row[index] = item.strip()\n",
    "        \n",
    "        for key in ethnicity_columns.keys():\n",
    "                if key in split_row:\n",
    "                    ethnicity_columns[key].append(1)\n",
    "                else:                    \n",
    "                    ethnicity_columns[key].append(0)\n",
    "\n",
    "for key, value in ethnicity_columns.items():\n",
    "    dating_data[key] = value\n",
    "    dating_data[key] = dating_data[key].astype('category')\n",
    "    \n",
    "print(dating_data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "coastal-utility",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age     status sex orientation       body_type               diet  \\\n",
      "0   22     single   m    straight  a little extra  strictly anything   \n",
      "1   35     single   m    straight         average       mostly other   \n",
      "2   38  available   m    straight            thin           anything   \n",
      "3   23     single   m    straight            thin         vegetarian   \n",
      "4   29     single   m    straight        athletic                NaN   \n",
      "\n",
      "     drinks      drugs                          education  \\\n",
      "0  socially      never      working on college/university   \n",
      "1     often  sometimes              working on space camp   \n",
      "2  socially        NaN     graduated from masters program   \n",
      "3  socially        NaN      working on college/university   \n",
      "4  socially      never  graduated from college/university   \n",
      "\n",
      "             ethnicity  ...  \\\n",
      "0         asian, white  ...   \n",
      "1                white  ...   \n",
      "2                  NaN  ...   \n",
      "3                white  ...   \n",
      "4  asian, black, other  ...   \n",
      "\n",
      "                                              speaks  asian white black other  \\\n",
      "0                                            english      1     1     0     0   \n",
      "1  english (fluently), spanish (poorly), french (...      0     1     0     0   \n",
      "2                               english, french, c++      0     0     0     0   \n",
      "3                           english, german (poorly)      0     1     0     0   \n",
      "4                                            english      1     0     1     1   \n",
      "\n",
      "  hispanic / latin pacific islander native american middle eastern indian  \n",
      "0                0                0               0              0      0  \n",
      "1                0                0               0              0      0  \n",
      "2                0                0               0              0      0  \n",
      "3                0                0               0              0      0  \n",
      "4                0                0               0              0      0  \n",
      "\n",
      "[5 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "print(dating_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "above-geneva",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now that we've one hot encoded our ethnicity category we can get rid of it entirely\n",
    "dating_data.drop('ethnicity',axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "increasing-finding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'(poorly),', '(okay),', '(poorly)', '(fluently),', '(fluently)', '(okay)'}\n",
      "67\n",
      "['afrikaans,', 'arabic,', 'basque,', 'belarusan,', 'bengali,', 'breton,', 'bulgarian,', 'c++,', 'catalan,', 'cebuano,', 'chechen,', 'chinese,', 'croatian,', 'czech,', 'danish,', 'dutch,', 'english,', 'esperanto,', 'estonian,', 'farsi,', 'finnish,', 'french,', 'frisian,', 'georgian,', 'german,', 'greek,', 'gujarati,', 'hawaiian,', 'hebrew,', 'hindi,', 'hungarian,', 'ilongo,', 'indonesian,', 'irish,', 'italian,', 'japanese,', 'khmer,', 'korean,', 'language,', 'latin,', 'lisp,', 'malay,', 'maori,', 'norwegian,', 'occitan,', 'other,', 'persian,', 'polish,', 'portuguese,', 'romanian,', 'rotuman,', 'russian,', 'sanskrit,', 'serbian,', 'slovak,', 'spanish,', 'swahili,', 'swedish,', 'tagalog,', 'tamil,', 'thai,', 'tibetan,', 'turkish,', 'ukrainian,', 'urdu,', 'vietnamese,', 'yiddish,']\n",
      "78\n",
      "afrikaans\n",
      "albanian\n",
      "ancient greek\n",
      "arabic\n",
      "armenian\n",
      "basque\n",
      "belarusan\n",
      "bengali\n",
      "breton\n",
      "bulgarian\n",
      "c++\n",
      "catalan\n",
      "cebuano\n",
      "chechen\n",
      "chinese\n",
      "croatian\n",
      "czech\n",
      "danish\n",
      "dutch\n",
      "english\n",
      "esperanto\n",
      "estonian\n",
      "farsi\n",
      "finnish\n",
      "french\n",
      "frisian\n",
      "georgian\n",
      "german\n",
      "greek\n",
      "gujarati\n",
      "hawaiian\n",
      "hebrew\n",
      "hindi\n",
      "hungarian\n",
      "icelandic\n",
      "ilongo\n",
      "indonesian\n",
      "irish\n",
      "italian\n",
      "japanese\n",
      "khmer\n",
      "korean\n",
      "latin\n",
      "latvian\n",
      "lisp\n",
      "lithuanian\n",
      "malay\n",
      "maori\n",
      "mongolian\n",
      "norwegian\n",
      "occitan\n",
      "other\n",
      "persian\n",
      "polish\n",
      "portuguese\n",
      "romanian\n",
      "rotuman\n",
      "russian\n",
      "sanskrit\n",
      "sardinian\n",
      "serbian\n",
      "sign\n",
      "sign language\n",
      "slovak\n",
      "slovenian\n",
      "spanish\n",
      "swahili\n",
      "swedish\n",
      "tagalog\n",
      "tamil\n",
      "thai\n",
      "tibetan\n",
      "turkish\n",
      "ukrainian\n",
      "urdu\n",
      "vietnamese\n",
      "welsh\n",
      "yiddish\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 10)\n",
    "import numbers\n",
    "languages_set = set()\n",
    "\n",
    "#Build set of uniques\n",
    "\n",
    "for entry in dating_data['speaks'].values:\n",
    "    if isinstance(entry, numbers.Number) is False:\n",
    "        if \" \" in entry:\n",
    "            split_list = entry.split()\n",
    "            for word in split_list:\n",
    "                languages_set.add(word)\n",
    "        else:\n",
    "            languages_set.add(entry)\n",
    "            \n",
    "anti_language_set = set()\n",
    "fluency_set = set()\n",
    "#build set of redundancies\n",
    "for language in languages_set:\n",
    "    if language[0] == \"(\":\n",
    "        fluency_set.add(language) \n",
    "    if language[-1] == ',':\n",
    "        anti_language_set.add(language) \n",
    "\n",
    "#remove redundancies        \n",
    "for unlanguage in anti_language_set:\n",
    "    languages_set.remove(unlanguage)\n",
    "for fluency in fluency_set:\n",
    "    if fluency in languages_set:\n",
    "        languages_set.remove(fluency)    \n",
    "    if fluency in anti_language_set:        \n",
    "        anti_language_set.remove(fluency)\n",
    "\n",
    "languages_set.remove('language')\n",
    "languages_set.remove('ancient')\n",
    "languages_set.add('ancient greek')\n",
    "languages_set.add('sign language')\n",
    "#display results        \n",
    "print(fluency_set)\n",
    "print(len(anti_language_set))\n",
    "#present redundant languages alphabetically\n",
    "ordered_unique_anti_languages = list(anti_language_set)\n",
    "ordered_unique_anti_languages =sorted(ordered_unique_anti_languages )\n",
    "print(ordered_unique_anti_languages )            \n",
    "print(len(languages_set))\n",
    "#present languages alphabetically\n",
    "ordered_unique_languages = list(languages_set)\n",
    "ordered_unique_languages = sorted(ordered_unique_languages)\n",
    "for language in ordered_unique_languages:\n",
    "    print(language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "consistent-mouth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "language,\n"
     ]
    }
   ],
   "source": [
    "#test to make sure our language list is complete. It is complete since this prints nothing\n",
    "for unlanguage in anti_language_set:\n",
    "    if unlanguage[:-1] not in languages_set:\n",
    "        print(unlanguage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "satisfied-conversation",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dating_data['speaks'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "involved-pickup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                              english\n",
      "1    english (fluently), spanish (poorly), french (...\n",
      "2                                 english, french, c++\n",
      "3                             english, german (poorly)\n",
      "4                                              english\n",
      "Name: speaks, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(dating_data['speaks'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "invalid-conditioning",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this was mostly so I could reference the data structure while I was working on the following section. Commented out because\n",
    "#when you view this on github you can't shrink the cell and it's absolutely gigantic.\n",
    "#for row in dating_data['speaks']:\n",
    "    #if type(row) != float:\n",
    "     #   print(row.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "french-vancouver",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RangeIndex(start=0, stop=59946, step=1)\n"
     ]
    }
   ],
   "source": [
    "print(dating_data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "isolated-sympathy",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'languages_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-b520c51049d5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#making dictionary of lists to become columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mlanguage_columns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0munique\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlanguages_set\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mlanguage_columns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'languages_set' is not defined"
     ]
    }
   ],
   "source": [
    "#Here we're making language columns with numerical fluency values\n",
    "\n",
    "\n",
    "#making dictionary of lists to become columns\n",
    "language_columns = {}\n",
    "for unique in languages_set:\n",
    "    language_columns[unique] = []\n",
    "\n",
    "language_df = pd.DataFrame)\n",
    "    \n",
    "    \n",
    "for row in dating_data['speaks']:\n",
    "    if type(row) != float:\n",
    "        split_row = row.split(',')\n",
    "        \n",
    "        for language in languages_set:\n",
    "            assignment_flag = 0\n",
    "            \n",
    "            for split_item in split_row:\n",
    "                if assignment_flag == 0:\n",
    "                    if language in split_item:\n",
    "                        if 'fluently' in split_item:\n",
    "                            language_columns[language].append(3)\n",
    "                            assignment_flag = 1\n",
    "                        elif 'okay' in split_item:\n",
    "                            language_columns[language].append(2)\n",
    "                            assignment_flag = 1\n",
    "                        elif 'poorly' in split_item:\n",
    "                            language_columns[language].append(1)\n",
    "                            assignment_flag = 1\n",
    "                        #we're assuming that if a fluency isn't specified they can be considered fluent. Domain knowledge suggests \n",
    "                        #non-specification indicates their native language.\n",
    "                        else:\n",
    "                            language_columns[language].append(3)\n",
    "                            assignment_flag = 1\n",
    "            if assignment_flag == 0:\n",
    "                language_columns[language].append(0)\n",
    "                        \n",
    "    else:\n",
    "        for language in languages_set:\n",
    "            language_columns[language].append(0)\n",
    "\n",
    "for key, value in language_columns.items():\n",
    "    dating_data[key] = value\n",
    "    dating_data[key] = dating_data[key].astype('category')\n",
    "    \n",
    "print(dating_data.dtypes)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "virgin-egypt",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now that we've one hot encoded our speaks column, we can get rid of it entirely.\n",
    "dating_data.drop('speaks',axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "practical-season",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "other                             7589\n",
      "student                           4882\n",
      "science / tech / engineering      4848\n",
      "computer / hardware / software    4709\n",
      "artistic / musical / writer       4439\n",
      "                                  ... \n",
      "rather not say                     436\n",
      "transportation                     366\n",
      "unemployed                         273\n",
      "retired                            250\n",
      "military                           204\n",
      "Name: job, Length: 21, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#checking out jobs\n",
    "print(dating_data['job'].value_counts())\n",
    "#This is obviously categories so we'll just convert it right over.\n",
    "dating_data['job'] = dating_data['job'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "sound-satisfaction",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('san francisco, california', 31064)\n",
      "('oakland, california', 7214)\n",
      "('berkeley, california', 4212)\n",
      "('san mateo, california', 1331)\n",
      "('palo alto, california', 1064)\n",
      "('alameda, california', 910)\n",
      "('san rafael, california', 755)\n",
      "('hayward, california', 747)\n",
      "('emeryville, california', 738)\n",
      "('redwood city, california', 693)\n",
      "('daly city, california', 681)\n",
      "('san leandro, california', 651)\n",
      "('walnut creek, california', 644)\n",
      "('vallejo, california', 558)\n",
      "('menlo park, california', 479)\n",
      "('richmond, california', 424)\n",
      "('south san francisco, california', 416)\n",
      "('mountain view, california', 384)\n",
      "('novato, california', 369)\n",
      "('burlingame, california', 361)\n",
      "('pleasant hill, california', 347)\n",
      "('castro valley, california', 345)\n",
      "('stanford, california', 341)\n",
      "('el cerrito, california', 325)\n",
      "('pacifica, california', 323)\n",
      "('martinez, california', 316)\n",
      "('mill valley, california', 315)\n",
      "('san bruno, california', 290)\n",
      "('san pablo, california', 245)\n",
      "('belmont, california', 243)\n",
      "('albany, california', 233)\n",
      "('san carlos, california', 227)\n",
      "('benicia, california', 203)\n",
      "('lafayette, california', 180)\n",
      "('sausalito, california', 179)\n",
      "('millbrae, california', 155)\n",
      "('san anselmo, california', 147)\n",
      "('el sobrante, california', 137)\n",
      "('san lorenzo, california', 135)\n",
      "('fairfax, california', 121)\n",
      "('hercules, california', 117)\n",
      "('pinole, california', 104)\n",
      "('half moon bay, california', 99)\n",
      "('fremont, california', 90)\n",
      "('green brae, california', 87)\n",
      "('orinda, california', 85)\n",
      "('moraga, california', 84)\n",
      "('larkspur, california', 80)\n",
      "('corte madera, california', 76)\n",
      "('belvedere tiburon, california', 57)\n",
      "('atherton, california', 45)\n",
      "('brisbane, california', 38)\n",
      "('rodeo, california', 37)\n",
      "('crockett, california', 32)\n",
      "('el granada, california', 27)\n",
      "('foster city, california', 24)\n",
      "('kentfield, california', 18)\n",
      "('woodacre, california', 16)\n",
      "('east palo alto, california', 13)\n",
      "('montara, california', 12)\n",
      "('ross, california', 12)\n",
      "('piedmont, california', 12)\n",
      "('woodside, california', 11)\n",
      "('westlake, california', 11)\n",
      "('los angeles, california', 10)\n",
      "('lagunitas, california', 10)\n",
      "('san geronimo, california', 9)\n",
      "('bolinas, california', 8)\n",
      "('point richmond, california', 8)\n",
      "('moss beach, california', 8)\n",
      "('west oakland, california', 7)\n",
      "('colma, california', 7)\n",
      "('san diego, california', 6)\n",
      "('santa cruz, california', 5)\n",
      "('tiburon, california', 5)\n",
      "('hillsborough, california', 4)\n",
      "('stinson beach, california', 4)\n",
      "('santa monica, california', 3)\n",
      "('bayshore, california', 3)\n",
      "('nicasio, california', 3)\n",
      "('san jose, california', 2)\n",
      "('petaluma, california', 2)\n",
      "('san quentin, california', 2)\n",
      "('sacramento, california', 2)\n",
      "('kensington, california', 2)\n",
      "('redwood shores, california', 2)\n",
      "('forest knolls, california', 2)\n",
      "('los gatos, california', 2)\n",
      "('santa rosa, california', 2)\n",
      "('irvine, california', 2)\n",
      "('napa, california', 2)\n",
      "('freedom, california', 1)\n",
      "('hacienda heights, california', 1)\n",
      "('riverside, california', 1)\n",
      "('rohnert park, california', 1)\n",
      "('canyon country, california', 1)\n",
      "('glencove, california', 1)\n",
      "('olema, california', 1)\n",
      "('union city, california', 1)\n",
      "('brea, california', 1)\n",
      "('santa clara, california', 1)\n",
      "('studio city, california', 1)\n",
      "('concord, california', 1)\n",
      "('seaside, california', 1)\n",
      "('magalia, california', 1)\n",
      "('orange, california', 1)\n",
      "('sunnyvale, california', 1)\n",
      "('ashland, california', 1)\n",
      "('pasadena, california', 1)\n",
      "('arcadia, california', 1)\n",
      "('milpitas, california', 1)\n",
      "('port costa, california', 1)\n",
      "('livingston, california', 1)\n",
      "('granite bay, california', 1)\n",
      "('isla vista, california', 1)\n",
      "('hilarita, california', 1)\n",
      "('campbell, california', 1)\n",
      "('santa ana, california', 1)\n",
      "('north hollywood, california', 1)\n",
      "('nevada city, california', 1)\n",
      "('stockton, california', 1)\n",
      "('marin city, california', 1)\n",
      "('waterford, california', 1)\n",
      "('muir beach, california', 1)\n",
      "('pacheco, california', 1)\n",
      "('canyon, california', 1)\n",
      "('oceanview, california', 1)\n",
      "('san luis obispo, california', 1)\n",
      "('modesto, california', 1)\n",
      "('costa mesa, california', 1)\n",
      "('oakley, california', 1)\n",
      "('chico, california', 1)\n",
      "('south lake tahoe, california', 1)\n",
      "('vacaville, california', 1)\n",
      "('long beach, california', 1)\n"
     ]
    }
   ],
   "source": [
    "#exploring location. Unsure if we want to turn this into categories or not.\n",
    "import collections\n",
    "#print(dating_data['location'].value_counts())\n",
    "location_counter_dict = collections.OrderedDict()\n",
    "for each in dating_data['location']:\n",
    "    if 'california' in each:\n",
    "        if each not in location_counter_dict:\n",
    "            location_counter_dict[each] = 0\n",
    "        location_counter_dict[each] += 1\n",
    "        \n",
    "sorted_california_locations = sorted(location_counter_dict.items(), key=lambda x: x[1], reverse= True)\n",
    "for each in sorted_california_locations:\n",
    "    print(each)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "legendary-muscle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('california', 59855), ('new york', 17), ('illinois', 8), ('massachusetts', 5), ('oregon', 4), ('michigan', 4), ('texas', 4), ('arizona', 3), ('florida', 3), ('colorado', 2), ('hawaii', 2), ('virginia', 2), ('spain', 2), ('united kingdom', 2), ('minnesota', 2), ('georgia', 2), ('utah', 2), ('washington', 2), ('district of columbia', 2), ('ohio', 2), ('montana', 1), ('wisconsin', 1), ('nevada', 1), ('vietnam', 1), ('ireland', 1), ('louisiana', 1), ('north carolina', 1), ('idaho', 1), ('mississippi', 1), ('new jersey', 1), ('west virginia', 1), ('connecticut', 1), ('tennessee', 1), ('rhode island', 1), ('british columbia', 1), ('missouri', 1), ('germany', 1), ('pennsylvania', 1), ('netherlands', 1), ('switzerland', 1), ('mexico', 1)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "count_dict = collections.OrderedDict()\n",
    "for row in dating_data['location']:\n",
    "    split_row = row.split(',')\n",
    "    stripped = split_row[1].strip()\n",
    "    if stripped not in count_dict:\n",
    "        count_dict[stripped] = 0\n",
    "    count_dict[stripped] += 1\n",
    "sorted_counts = sorted(count_dict.items(), key=lambda x: x[1], reverse= True)\n",
    "print(sorted_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "engaging-bedroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above is showing me that the city parameter is only really relevant within california... buuut most of my data\n",
    "# exists within california\n",
    "\n",
    "#given that we want to retain *some* utility from this column I think we're going to split it into a city column and a \n",
    "# state/country column. Sorry to all our non-california data-points but you're statistically insignificant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "warming-jason",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>status</th>\n",
       "      <th>sex</th>\n",
       "      <th>orientation</th>\n",
       "      <th>body_type</th>\n",
       "      <th>diet</th>\n",
       "      <th>drinks</th>\n",
       "      <th>drugs</th>\n",
       "      <th>education</th>\n",
       "      <th>height</th>\n",
       "      <th>...</th>\n",
       "      <th>maori</th>\n",
       "      <th>vietnamese</th>\n",
       "      <th>tibetan</th>\n",
       "      <th>rotuman</th>\n",
       "      <th>frisian</th>\n",
       "      <th>chinese</th>\n",
       "      <th>czech</th>\n",
       "      <th>polish</th>\n",
       "      <th>city</th>\n",
       "      <th>state_or_country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22</td>\n",
       "      <td>single</td>\n",
       "      <td>m</td>\n",
       "      <td>straight</td>\n",
       "      <td>a little extra</td>\n",
       "      <td>strictly anything</td>\n",
       "      <td>socially</td>\n",
       "      <td>never</td>\n",
       "      <td>working on college/university</td>\n",
       "      <td>75.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>south san francisco</td>\n",
       "      <td>california</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>35</td>\n",
       "      <td>single</td>\n",
       "      <td>m</td>\n",
       "      <td>straight</td>\n",
       "      <td>average</td>\n",
       "      <td>mostly other</td>\n",
       "      <td>often</td>\n",
       "      <td>sometimes</td>\n",
       "      <td>working on space camp</td>\n",
       "      <td>70.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>oakland</td>\n",
       "      <td>california</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>available</td>\n",
       "      <td>m</td>\n",
       "      <td>straight</td>\n",
       "      <td>thin</td>\n",
       "      <td>anything</td>\n",
       "      <td>socially</td>\n",
       "      <td>NaN</td>\n",
       "      <td>graduated from masters program</td>\n",
       "      <td>68.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>san francisco</td>\n",
       "      <td>california</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23</td>\n",
       "      <td>single</td>\n",
       "      <td>m</td>\n",
       "      <td>straight</td>\n",
       "      <td>thin</td>\n",
       "      <td>vegetarian</td>\n",
       "      <td>socially</td>\n",
       "      <td>NaN</td>\n",
       "      <td>working on college/university</td>\n",
       "      <td>71.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>berkeley</td>\n",
       "      <td>california</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29</td>\n",
       "      <td>single</td>\n",
       "      <td>m</td>\n",
       "      <td>straight</td>\n",
       "      <td>athletic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>socially</td>\n",
       "      <td>never</td>\n",
       "      <td>graduated from college/university</td>\n",
       "      <td>66.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>san francisco</td>\n",
       "      <td>california</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59941</th>\n",
       "      <td>59</td>\n",
       "      <td>single</td>\n",
       "      <td>f</td>\n",
       "      <td>straight</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>socially</td>\n",
       "      <td>never</td>\n",
       "      <td>graduated from college/university</td>\n",
       "      <td>62.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>oakland</td>\n",
       "      <td>california</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59942</th>\n",
       "      <td>24</td>\n",
       "      <td>single</td>\n",
       "      <td>m</td>\n",
       "      <td>straight</td>\n",
       "      <td>fit</td>\n",
       "      <td>mostly anything</td>\n",
       "      <td>often</td>\n",
       "      <td>sometimes</td>\n",
       "      <td>working on college/university</td>\n",
       "      <td>72.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>san francisco</td>\n",
       "      <td>california</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59943</th>\n",
       "      <td>42</td>\n",
       "      <td>single</td>\n",
       "      <td>m</td>\n",
       "      <td>straight</td>\n",
       "      <td>average</td>\n",
       "      <td>mostly anything</td>\n",
       "      <td>not at all</td>\n",
       "      <td>never</td>\n",
       "      <td>graduated from masters program</td>\n",
       "      <td>71.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>south san francisco</td>\n",
       "      <td>california</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59944</th>\n",
       "      <td>27</td>\n",
       "      <td>single</td>\n",
       "      <td>m</td>\n",
       "      <td>straight</td>\n",
       "      <td>athletic</td>\n",
       "      <td>mostly anything</td>\n",
       "      <td>socially</td>\n",
       "      <td>often</td>\n",
       "      <td>working on college/university</td>\n",
       "      <td>73.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>san francisco</td>\n",
       "      <td>california</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59945</th>\n",
       "      <td>39</td>\n",
       "      <td>single</td>\n",
       "      <td>m</td>\n",
       "      <td>gay</td>\n",
       "      <td>average</td>\n",
       "      <td>NaN</td>\n",
       "      <td>socially</td>\n",
       "      <td>NaN</td>\n",
       "      <td>graduated from masters program</td>\n",
       "      <td>68.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>san francisco</td>\n",
       "      <td>california</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>59946 rows Ã— 105 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age     status sex orientation       body_type               diet  \\\n",
       "0       22     single   m    straight  a little extra  strictly anything   \n",
       "1       35     single   m    straight         average       mostly other   \n",
       "2       38  available   m    straight            thin           anything   \n",
       "3       23     single   m    straight            thin         vegetarian   \n",
       "4       29     single   m    straight        athletic                NaN   \n",
       "...    ...        ...  ..         ...             ...                ...   \n",
       "59941   59     single   f    straight             NaN                NaN   \n",
       "59942   24     single   m    straight             fit    mostly anything   \n",
       "59943   42     single   m    straight         average    mostly anything   \n",
       "59944   27     single   m    straight        athletic    mostly anything   \n",
       "59945   39     single   m         gay         average                NaN   \n",
       "\n",
       "           drinks      drugs                          education  height  ...  \\\n",
       "0        socially      never      working on college/university    75.0  ...   \n",
       "1           often  sometimes              working on space camp    70.0  ...   \n",
       "2        socially        NaN     graduated from masters program    68.0  ...   \n",
       "3        socially        NaN      working on college/university    71.0  ...   \n",
       "4        socially      never  graduated from college/university    66.0  ...   \n",
       "...           ...        ...                                ...     ...  ...   \n",
       "59941    socially      never  graduated from college/university    62.0  ...   \n",
       "59942       often  sometimes      working on college/university    72.0  ...   \n",
       "59943  not at all      never     graduated from masters program    71.0  ...   \n",
       "59944    socially      often      working on college/university    73.0  ...   \n",
       "59945    socially        NaN     graduated from masters program    68.0  ...   \n",
       "\n",
       "       maori vietnamese tibetan rotuman frisian chinese czech polish  \\\n",
       "0          0          0       0       0       0       0     0      0   \n",
       "1          0          0       0       0       0       0     0      0   \n",
       "2          0          0       0       0       0       0     0      0   \n",
       "3          0          0       0       0       0       0     0      0   \n",
       "4          0          0       0       0       0       0     0      0   \n",
       "...      ...        ...     ...     ...     ...     ...   ...    ...   \n",
       "59941      0          0       0       0       0       0     0      0   \n",
       "59942      0          0       0       0       0       0     0      0   \n",
       "59943      0          0       0       0       0       0     0      0   \n",
       "59944      0          0       0       0       0       1     0      0   \n",
       "59945      0          0       0       0       0       0     0      0   \n",
       "\n",
       "                      city state_or_country  \n",
       "0      south san francisco       california  \n",
       "1                  oakland       california  \n",
       "2            san francisco       california  \n",
       "3                 berkeley       california  \n",
       "4            san francisco       california  \n",
       "...                    ...              ...  \n",
       "59941              oakland       california  \n",
       "59942        san francisco       california  \n",
       "59943  south san francisco       california  \n",
       "59944        san francisco       california  \n",
       "59945        san francisco       california  \n",
       "\n",
       "[59946 rows x 105 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city = []\n",
    "state_or_country = []\n",
    "\n",
    "for row in dating_data['location']:\n",
    "    split_row = row.split(',')\n",
    "    city.append(split_row[0].strip())\n",
    "    state_or_country.append(split_row[1].strip())\n",
    "    \n",
    "dating_data['city'] = city\n",
    "dating_data['state_or_country'] = state_or_country\n",
    "dating_data.drop('location', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "mobile-format",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age                    int64\n",
      "status                object\n",
      "sex                 category\n",
      "orientation         category\n",
      "body_type           category\n",
      "                      ...   \n",
      "chinese             category\n",
      "czech               category\n",
      "polish              category\n",
      "city                  object\n",
      "state_or_country      object\n",
      "Length: 106, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(dating_data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "demographic-period",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "body_type 5296\n",
      "['a little extra', 'average', 'thin', 'athletic', 'fit', ..., 'full figured', 'jacked', 'rather not say', 'used up', 'overweight']\n",
      "Length: 13\n",
      "Categories (12, object): ['a little extra', 'average', 'thin', 'athletic', ..., 'jacked', 'rather not say', 'used up', 'overweight']\n",
      " \n",
      "diet 24395\n",
      "['strictly anything', 'mostly other', 'anything', 'vegetarian', NaN, ..., 'mostly kosher', 'strictly halal', 'halal', 'strictly kosher', 'kosher']\n",
      "Length: 19\n",
      "Categories (18, object): ['strictly anything', 'mostly other', 'anything', 'vegetarian', ..., 'strictly halal', 'halal', 'strictly kosher', 'kosher']\n",
      " \n",
      "drinks 2985\n",
      "['socially', 'often', 'not at all', 'rarely', NaN, 'very often', 'desperately']\n",
      "Categories (6, object): ['socially', 'often', 'not at all', 'rarely', 'very often', 'desperately']\n",
      " \n",
      "drugs 14080\n",
      "['never', 'sometimes', NaN, 'often']\n",
      "Categories (3, object): ['never', 'sometimes', 'often']\n",
      " \n",
      "education 6628\n",
      "['working on college/university', 'working on space camp', 'graduated from masters program', 'graduated from college/university', 'working on two-year college', ..., 'law school', 'dropped out of masters program', 'ph.d program', 'dropped out of law school', 'med school']\n",
      "Length: 33\n",
      "Categories (32, object): ['working on college/university', 'working on space camp', 'graduated from masters program', 'graduated from college/university', ..., 'dropped out of masters program', 'ph.d program', 'dropped out of law school', 'med school']\n",
      " \n",
      "height 3\n",
      "[75. 70. 68. 71. 66. 67. 65. 72. 62. 64. 69. 73. 74. 60. 63. 76. 61. 78.\n",
      " 79. 59. 80. 91. 83. 77. 58. 56. 95. 57. 87. 81. 36. 43. 52. 55. 53. 93.\n",
      "  8. 54. 82.  3. 86. 42. 84. 94. 50.  6. 47. 49. 48. 90. 88. nan 37.  9.\n",
      " 51.  1. 92. 26. 85. 89.  4.]\n",
      " \n",
      "job 8198\n",
      "['transportation', 'hospitality / travel', NaN, 'student', 'artistic / musical / writer', ..., 'political / government', 'law / legal services', 'unemployed', 'military', 'retired']\n",
      "Length: 22\n",
      "Categories (21, object): ['transportation', 'hospitality / travel', 'student', 'artistic / musical / writer', ..., 'law / legal services', 'unemployed', 'military', 'retired']\n",
      " \n",
      "offspring 35561\n",
      "['doesn't have kids, but might want them', NaN, 'doesn't want kids', 'doesn't have kids, but wants them', 'doesn't have kids', ..., 'has a kid, and wants more', 'has kids, and might want more', 'might want kids', 'has a kid, and might want more', 'has kids, and wants more']\n",
      "Length: 16\n",
      "Categories (15, object): ['doesn't have kids, but might want them', 'doesn't want kids', 'doesn't have kids, but wants them', 'doesn't have kids', ..., 'has kids, and might want more', 'might want kids', 'has a kid, and might want more', 'has kids, and wants more']\n",
      " \n",
      "pets 19921\n",
      "['likes dogs and likes cats', 'has cats', 'likes cats', NaN, 'has dogs and likes cats', ..., 'dislikes dogs and has cats', 'dislikes dogs and dislikes cats', 'dislikes cats', 'dislikes dogs and likes cats', 'dislikes dogs']\n",
      "Length: 16\n",
      "Categories (15, object): ['likes dogs and likes cats', 'has cats', 'likes cats', 'has dogs and likes cats', ..., 'dislikes dogs and dislikes cats', 'dislikes cats', 'dislikes dogs and likes cats', 'dislikes dogs']\n",
      " \n",
      "religion 20226\n",
      "['agnosticism and very serious about it', 'agnosticism but not too serious about it', NaN, 'atheism', 'christianity', ..., 'hinduism and somewhat serious about it', 'islam but not too serious about it', 'buddhism and very serious about it', 'islam and laughing about it', 'islam and somewhat serious about it']\n",
      "Length: 46\n",
      "Categories (45, object): ['agnosticism and very serious about it', 'agnosticism but not too serious about it', 'atheism', 'christianity', ..., 'islam but not too serious about it', 'buddhism and very serious about it', 'islam and laughing about it', 'islam and somewhat serious about it']\n",
      " \n",
      "smokes 5512\n",
      "['sometimes', 'no', NaN, 'when drinking', 'yes', 'trying to quit']\n",
      "Categories (5, object): ['sometimes', 'no', 'when drinking', 'yes', 'trying to quit']\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for each in dating_data.columns:\n",
    "    nantotal = dating_data[each].isna().sum()\n",
    "    if nantotal > 0:\n",
    "        print(each, nantotal)\n",
    "        print(dating_data[each].unique())\n",
    "        print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "academic-treasure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bodytype 5296\n",
      "bodytype 0\n"
     ]
    }
   ],
   "source": [
    "#Dealing with body type NaNs. There's already a \"rather not say\" category. I believe that a NaN value indicates that they just \n",
    "#didn't answer the question, which I'm comfortable imputing as 'rather not say', since they chose not to say and body type is \n",
    "#a question asked early on in the profile creation process.\n",
    "print('bodytype', dating_data['body_type'].isna().sum())\n",
    "dating_data['body_type'].fillna('rather not say', inplace=True)\n",
    "print('bodytype', dating_data['body_type'].isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "linear-turkey",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diet 24395\n",
      "diet 0\n"
     ]
    }
   ],
   "source": [
    "#Dealing with diet next. #Here I'm going to assume that if you didn't specify your diet it wasn't important to you. Which means \n",
    "# you're probably operating under the default american diet (most of our data comes from california), which means anything.\n",
    "print('diet', dating_data['diet'].isna().sum())    \n",
    "dating_data['diet'].fillna('anything', inplace=True)\n",
    "print('diet', dating_data['diet'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bacterial-amino",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drinks 2985\n",
      "drinks 0\n"
     ]
    }
   ],
   "source": [
    "#Next we're goign to look at drinks Here it's a little less clear what to impute. Stances on drinking can vary pretty wildly. \n",
    "#I'm going to take a safe position and make a new category 'unspecified' and call it good. Not answering the question might\n",
    "#be a predictive behavior, who knows?\n",
    "print('drinks', dating_data['drinks'].isna().sum())\n",
    "dating_data['drinks'].cat.add_categories('unspecified', inplace=True)\n",
    "dating_data['drinks'].fillna('unspecified', inplace=True)\n",
    "print('drinks', dating_data['drinks'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ambient-flashing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drugs 14080\n",
      "drugs 0\n"
     ]
    }
   ],
   "source": [
    "#drugs next, here we're going to take the same approach as with drinking. Not specifying is it's own choice.\n",
    "print('drugs', dating_data['drugs'].isna().sum())\n",
    "dating_data['drugs'].cat.add_categories('unspecified', inplace=True)\n",
    "dating_data['drugs'].fillna('unspecified', inplace=True)\n",
    "print('drugs', dating_data['drugs'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "continent-pixel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "education 6628\n",
      "education 0\n"
     ]
    }
   ],
   "source": [
    "#and now to education. We're going to default to unspecificed again. Education is important to a lot of people, not specifying \n",
    "#it is a choice, and it very well could be predictive.\n",
    "print('education', dating_data['education'].isna().sum())\n",
    "dating_data['education'].cat.add_categories('unspecified', inplace=True)\n",
    "dating_data['education'].fillna('unspecified', inplace=True)\n",
    "print('education', dating_data['education'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "nasty-contractor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "height 3\n",
      "height 0\n"
     ]
    }
   ],
   "source": [
    "#next up is height. I'm going to drop the rows with NaN here, there's only three of them\n",
    "print('height', dating_data['height'].isna().sum())\n",
    "dating_data.dropna(subset=['height'], inplace=True)\n",
    "print('height', dating_data['height'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "distinguished-estonia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job 8196\n",
      "job 0\n"
     ]
    }
   ],
   "source": [
    "#Jobs are up to bat! We're going to take advantage of the preexisting 'rather not say' category here. Again, employment is \n",
    "#important to most americans, choosing not to specify it is a noteworthy choice on a dating profile.\n",
    "print('job', dating_data['job'].isna().sum())\n",
    "dating_data['job'].fillna('rather not say', inplace=True)\n",
    "print('job', dating_data['job'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "institutional-steering",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "offspring 35559\n",
      "offspring 0\n"
     ]
    }
   ],
   "source": [
    "# offspring now # we have a ton of NaNs here. Enough that it makes me skeptical of this column being predictive. \n",
    "# I defienitely can't drop rows with NaN here and I'm not quite ready to drop the column entirely, so we're going to go with \n",
    "# our trusty 'unspecified' category\n",
    "print('offspring', dating_data['offspring'].isna().sum())\n",
    "dating_data['offspring'].cat.add_categories('unspecified', inplace=True)\n",
    "dating_data['offspring'].fillna('unspecified', inplace=True)\n",
    "print('offspring', dating_data['offspring'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "brutal-concert",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pets 19919\n",
      "pets 0\n"
     ]
    }
   ],
   "source": [
    "#pets. Same problem and logic as offspring. Too many to just drop, and even though there's enough NaNs to make me question the \n",
    "#predictive power of the column as a whole I don't want to drop it wholesale, so 'unspecified' yet again\n",
    "print('pets', dating_data['pets'].isna().sum())\n",
    "dating_data['pets'].cat.add_categories('unspecified', inplace=True)\n",
    "dating_data['pets'].fillna('unspecified', inplace=True)\n",
    "print('pets', dating_data['pets'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "threatened-vector",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "religion 20223\n",
      "religion 0\n"
     ]
    }
   ],
   "source": [
    "# religion. Same logic/problem as before. There's a preexisting other category, but I think that's significantly different from \n",
    "# not specifiying, so we're going to throw in unspecified again\n",
    "print('religion', dating_data['religion'].isna().sum())\n",
    "dating_data['religion'].cat.add_categories('unspecified', inplace=True)\n",
    "dating_data['religion'].fillna('unspecified', inplace=True)\n",
    "print('religion', dating_data['religion'].isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "reasonable-manitoba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59943, 106)\n",
      "smokes 5510\n",
      "smokes 0\n",
      "(54433, 106)\n"
     ]
    }
   ],
   "source": [
    "#smokes. This is the variable we're trying to predict. Makes \n",
    "print(dating_data.shape)\n",
    "print('smokes', dating_data['smokes'].isna().sum())\n",
    "dating_data.dropna(subset=['smokes'], inplace=True)\n",
    "print('smokes', dating_data['smokes'].isna().sum())\n",
    "print(dating_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "temporal-installation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "    \n",
    "    \n",
    "# This utility was created because students were getting confused when they ran \n",
    "# their notebooks twice, the previous write-to-file code would do nothing and say  \n",
    "# nothing. The students thought the file was over-written when in fact, it was not -\n",
    "# generating hidden bugs in subsequent notebooks.\n",
    "\n",
    "def save_file(data, fname, dname):\n",
    "    \"\"\"Save a datafile (data) to a specific location (dname) and filename (fname)\n",
    "    \n",
    "    Currently valid formats are limited to CSV or PKL.\"\"\"\n",
    "    \n",
    "    if not os.path.exists(dname):\n",
    "        os.mkdir(dname)\n",
    "        print(f'Directory {dname} was created.')\n",
    "        \n",
    "    fpath = os.path.join(dname, fname)\n",
    "    \n",
    "    \n",
    "    if os.path.exists(fpath):\n",
    "        print(\"A file already exists with this name.\\n\")\n",
    "\n",
    "        yesno = None\n",
    "        while yesno != \"Y\" and yesno != \"N\":\n",
    "            yesno = input('Do you want to overwrite? (Y/N)').strip()[0].capitalize()\n",
    "            if yesno == \"Y\":\n",
    "                print(f'Writing file.  \"{fpath}\"')\n",
    "                _save_file(data, fpath)\n",
    "                break  # Not required\n",
    "            elif yesno == \"N\":\n",
    "                print('\\nPlease re-run this cell with a new filename.')\n",
    "                break  # Not required\n",
    "            else:\n",
    "                print('\\nUnknown input, please enter \"Y\" or \"N\".')\n",
    "\n",
    "    else:  # path does not exist, ok to save the file\n",
    "        print(f'Writing file.  \"{fpath}\"')\n",
    "        _save_file(data, fpath)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "def _save_file(data, fpath):\n",
    "    valid_ftypes = ['.csv', '.pkl']\n",
    "    \n",
    "    assert (fpath[-4:] in valid_ftypes), \"Invalid file type.  Use '.csv' or '.pkl'\"\n",
    "\n",
    "    # Figure out what kind of file we're dealing with by name\n",
    "    if fpath[-3:] == 'csv':\n",
    "        data.to_csv(fpath, index=False)\n",
    "    elif fpath[-3:] == 'pkl':\n",
    "        with open(fpath, 'wb') as f:\n",
    "            pickle.dump(data, f)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "empty-breed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A file already exists with this name.\n",
      "\n",
      "Do you want to overwrite? (Y/N)Y\n",
      "Writing file.  \"derived_data\\wrangled_dating_data.csv\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "save_file(dating_data, 'wrangled_dating_data.csv', 'derived_data')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
