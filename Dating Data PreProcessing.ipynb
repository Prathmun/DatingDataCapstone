{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "early-reality",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "determined-mainland",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Users/Prathmun/Documents/Springboard Jupyter/Capstone 2/DataDataCapstone/derived_data/wrangled_dating_data.csv')\n",
    "df_ethnicity = pd.read_csv('C:/Users/Prathmun/Documents/Springboard Jupyter/Capstone 2/DataDataCapstone/derived_data/wrangled_ethnicity_data.csv')\n",
    "df_language = pd.read_csv('C:/Users/Prathmun/Documents/Springboard Jupyter/Capstone 2/DataDataCapstone/derived_data/wrangled_language_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ethical-pharmacy",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54433, 15)\n",
      "age              int64\n",
      "status          object\n",
      "sex             object\n",
      "orientation     object\n",
      "body_type       object\n",
      "diet            object\n",
      "drinks          object\n",
      "drugs           object\n",
      "education       object\n",
      "height         float64\n",
      "job             object\n",
      "pets            object\n",
      "religion        object\n",
      "sign            object\n",
      "smokes          object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Next three boxes we're just checking the integrity of our dataframe and reminding ourself of which columns need to be scaled\n",
    "print(df.shape)\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "standard-roberts",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59946, 10)\n",
      "asian                     int64\n",
      "white                     int64\n",
      "black                     int64\n",
      "other                     int64\n",
      "hispanic / latin          int64\n",
      "pacific islander          int64\n",
      "native american           int64\n",
      "middle eastern            int64\n",
      "indian                    int64\n",
      "ethnicity_multiplicity    int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df_ethnicity.shape)\n",
    "print(df_ethnicity.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "forced-hunter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59946, 80)\n",
      "japanese               int64\n",
      "tagalog                int64\n",
      "maori                  int64\n",
      "arabic                 int64\n",
      "latin                  int64\n",
      "                      ...   \n",
      "lisp                   int64\n",
      "norwegian              int64\n",
      "fluency_score        float64\n",
      "multifluent_score      int64\n",
      "num_spoken             int64\n",
      "Length: 80, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df_language.shape)\n",
    "print(df_language.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "banned-fifteen",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_be_scaled = ['age', 'height', 'ethnicity_multiplicity', 'multifluent_score', 'num_spoken']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "subtle-harmony",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59946, 105)\n",
      "    age     status sex orientation       body_type               diet  \\\n",
      "0  22.0     single   m    straight  a little extra  strictly anything   \n",
      "1  35.0     single   m    straight         average       mostly other   \n",
      "2  38.0  available   m    straight            thin           anything   \n",
      "3  23.0     single   m    straight            thin         vegetarian   \n",
      "4  29.0     single   m    straight        athletic           anything   \n",
      "\n",
      "     drinks        drugs                          education  height  ...  \\\n",
      "0  socially        never      working on college/university    75.0  ...   \n",
      "1     often    sometimes              working on space camp    70.0  ...   \n",
      "2  socially  unspecified     graduated from masters program    68.0  ...   \n",
      "3  socially  unspecified      working on college/university    71.0  ...   \n",
      "4  socially        never  graduated from college/university    66.0  ...   \n",
      "\n",
      "  rotuman polish lithuanian danish spanish  lisp  norwegian  fluency_score  \\\n",
      "0       0      0          0      0       0     0          0            1.0   \n",
      "1       0      0          0      0       1     0          0            1.0   \n",
      "2       0      0          0      0       0     0          0            3.0   \n",
      "3       0      0          0      0       0     0          0            1.0   \n",
      "4       0      0          0      0       0     0          0            1.0   \n",
      "\n",
      "   multifluent_score  num_spoken  \n",
      "0                  3           1  \n",
      "1                  5           3  \n",
      "2                  9           3  \n",
      "3                  4           2  \n",
      "4                  3           1  \n",
      "\n",
      "[5 rows x 105 columns]\n"
     ]
    }
   ],
   "source": [
    "df_pre_split = pd.concat([df, df_ethnicity, df_language], axis=1)\n",
    "print(df_pre_split.shape)\n",
    "print(df_pre_split.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "above-timber",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I split out the language and ethnicity data frames before I dropped some of my NAN heavy rows. Going to just update this final dataframe by dropping those rows.\n",
    "df_pre_split= df_pre_split.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "aa6cc6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df_pre_split.drop('smokes', axis=1)    \n",
    "y=df_pre_split.smokes      \n",
    "X_dummy_thicc = pd.get_dummies(X)\n",
    "y_dummy_thicc=pd.get_dummies(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "df1b23ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making the split! The test_size is arbitrary for now, we'll adjust as our model(s) suggest.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_dummy_thicc, y_dummy_thicc, test_size=0.25, random_state=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "573fc8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Scaley_The_Standard_Scaler= StandardScaler()\n",
    "X_train = Scaley_The_Standard_Scaler.fit_transform(X_train[columns_to_be_scaled])\n",
    "X_test = Scaley_The_Standard_Scaler.transform(X_test[columns_to_be_scaled])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b905d04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "    \n",
    "    \n",
    "# This utility was created because students were getting confused when they ran \n",
    "# their notebooks twice, the previous write-to-file code would do nothing and say  \n",
    "# nothing. The students thought the file was over-written when in fact, it was not -\n",
    "# generating hidden bugs in subsequent notebooks.\n",
    "\n",
    "def save_file(data, fname, dname):\n",
    "    \"\"\"Save a datafile (data) to a specific location (dname) and filename (fname)\n",
    "    \n",
    "    Currently valid formats are limited to CSV or PKL.\"\"\"\n",
    "    \n",
    "    if not os.path.exists(dname):\n",
    "        os.mkdir(dname)\n",
    "        print(f'Directory {dname} was created.')\n",
    "        \n",
    "    fpath = os.path.join(dname, fname)\n",
    "    \n",
    "    \n",
    "    if os.path.exists(fpath):\n",
    "        print(\"A file already exists with this name.\\n\")\n",
    "\n",
    "        yesno = None\n",
    "        while yesno != \"Y\" and yesno != \"N\":\n",
    "            yesno = input('Do you want to overwrite? (Y/N)').strip()[0].capitalize()\n",
    "            if yesno == \"Y\":\n",
    "                print(f'Writing file.  \"{fpath}\"')\n",
    "                _save_file(data, fpath)\n",
    "                break  # Not required\n",
    "            elif yesno == \"N\":\n",
    "                print('\\nPlease re-run this cell with a new filename.')\n",
    "                break  # Not required\n",
    "            else:\n",
    "                print('\\nUnknown input, please enter \"Y\" or \"N\".')\n",
    "\n",
    "    else:  # path does not exist, ok to save the file\n",
    "        print(f'Writing file.  \"{fpath}\"')\n",
    "        _save_file(data, fpath)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "def _save_file(data, fpath):\n",
    "    valid_ftypes = ['.csv', '.pkl']\n",
    "    \n",
    "    assert (fpath[-4:] in valid_ftypes), \"Invalid file type.  Use '.csv' or '.pkl'\"\n",
    "\n",
    "    # Figure out what kind of file we're dealing with by name\n",
    "    if fpath[-3:] == 'csv':\n",
    "        data.to_csv(fpath, index=False)\n",
    "    elif fpath[-3:] == 'pkl':\n",
    "        with open(fpath, 'wb') as f:\n",
    "            pickle.dump(data, f)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d203267d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A file already exists with this name.\n",
      "\n",
      "Writing file.  \"derived_data\\X_train_TS25_RS3.csv\"\n",
      "A file already exists with this name.\n",
      "\n",
      "Writing file.  \"derived_data\\X_test_TS25_RS3.csv\"\n",
      "A file already exists with this name.\n",
      "\n",
      "Writing file.  \"derived_data\\y_train_TS25_RS3.csv\"\n",
      "A file already exists with this name.\n",
      "\n",
      "Writing file.  \"derived_data\\y_test_TS25_RS3.csv\"\n"
     ]
    }
   ],
   "source": [
    "save_file(pd.DataFrame(X_train), 'X_train_TS25_RS3.csv', 'derived_data')\n",
    "save_file(pd.DataFrame(X_test), 'X_test_TS25_RS3.csv', 'derived_data')\n",
    "save_file(pd.DataFrame(y_train), 'y_train_TS25_RS3.csv', 'derived_data')\n",
    "save_file(pd.DataFrame(y_test), 'y_test_TS25_RS3.csv', 'derived_data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341265bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "53a481214c1eee3a60e2d0cf4c920992715a8f8087f54faa7a39dc4a3c3b8bd0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
